{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up to date with 'origin/master'.\r\n",
      "\r\n",
      "nothing to commit, working tree clean\r\n"
     ]
    }
   ],
   "source": [
    "# ! ln ../bcc/inputs/163_A1*.npy inputs\n",
    "# ! ln ../bcc/gnn_models/4.model.pth models/tumor_map_gnn.pth\n",
    "# ! ln ../bcc/pretrain_model.pth models/tumor_map_cnn.pth\n",
    "# ! ln ../fat_dermis_epi_sq_model/v2/checkpoints_tissue_seg/104.checkpoint.pth models/macro_map_cnn.pth\n",
    "# ! ln ../nuclei_pipeline/seg_model/27.checkpoint.pth models/nuclei.pth\n",
    "# torch.save(GCNNet(2048, 4, [32]*3).state_dict(),\"models/macro_map_gnn.pth\")\n",
    "# ! pip uninstall pathpretrain -y && pip install git+https://github.com/jlevy44/PathPretrain\n",
    "! cd ../ArcticAI_Prototype/ && git add * */* && git commit -a -m \"workflow update\" && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm\n",
    "import numpy as np, pandas as pd\n",
    "from pathflowai.utils import generate_tissue_mask\n",
    "from itertools import product\n",
    "from scipy.ndimage.morphology import binary_fill_holes as fill_holes\n",
    "\n",
    "def preprocess(basename=\"163_A1a\",\n",
    "               threshold=0.05,\n",
    "               patch_size=256):\n",
    "    \n",
    "    image=f\"inputs/{basename}.npy\"\n",
    "    basename=os.path.basename(image).replace('.npy','')\n",
    "    image=np.load(image)\n",
    "    \n",
    "    masks=dict()\n",
    "    masks['tumor_map']=generate_tissue_mask(image,\n",
    "                             compression=10,\n",
    "                             otsu=False,\n",
    "                             threshold=240,\n",
    "                             connectivity=8,\n",
    "                             kernel=5,\n",
    "                             min_object_size=100000,\n",
    "                             return_convex_hull=False,\n",
    "                             keep_holes=False,\n",
    "                             max_hole_size=6000,\n",
    "                             gray_before_close=True,\n",
    "                             blur_size=51) \n",
    "    x_max,y_max=masks['tumor_map'].shape\n",
    "    masks['macro_map']=fill_holes(masks['tumor_map'])\n",
    "    \n",
    "    patch_info=dict()\n",
    "    for k in masks:\n",
    "        patch_info[k]=pd.DataFrame([[basename,x,y,patch_size,\"0\"] for x,y in tqdm.tqdm(list(product(range(0,x_max-patch_size,patch_size),range(0,y_max-patch_size,patch_size))))],columns=['ID','x','y','patch_size','annotation'])\n",
    "        patches=np.stack([image[x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())])                   \n",
    "        include_patches=np.stack([masks[k][x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())]).mean((1,2))>=threshold\n",
    "\n",
    "        np.save(f\"masks/{basename}_{k}.npy\",masks[k])\n",
    "        np.save(f\"patches/{basename}_{k}.npy\",patches[include_patches]) \n",
    "        patch_info[k].iloc[include_patches].to_pickle(f\"patches/{basename}_{k}.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(basename=\"163_A1a\",\n",
    "#                threshold=0.05,\n",
    "#                patch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model\n",
    "import os, torch, tqdm, pandas as pd, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # load using saved patches and mask file\n",
    "    def __init__(self, patch_info, npy_file, transform):\n",
    "        self.X=np.load(npy_file)\n",
    "        self.patch_info=pd.read_pickle(patch_info)\n",
    "        self.xy=self.patch_info[['x','y']].values\n",
    "        self.patch_size=self.patch_info['patch_size'].iloc[0]\n",
    "        self.length=self.patch_info.shape[0]\n",
    "        self.transform=transform\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.ID=os.path.basename(npy_file).replace(\".npy\",\"\")\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        x,y=self.xy[i]\n",
    "        return self.transform(self.to_pil(self.X[i]))#[x:x+patch_size,y:y+patch_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def embed(self,model,batch_size,out_dir):\n",
    "        Z=[]\n",
    "        dataloader=DataLoader(self,batch_size=batch_size,shuffle=False)\n",
    "        n_batches=len(self)//batch_size\n",
    "        with torch.no_grad():\n",
    "            for i,X in tqdm.tqdm(enumerate(dataloader),total=n_batches):\n",
    "                if torch.cuda.is_available(): X=X.cuda()\n",
    "                z=model(X).detach().cpu().numpy()\n",
    "                Z.append(z)\n",
    "        Z=np.vstack(Z)\n",
    "        torch.save(dict(embeddings=Z,patch_info=self.patch_info),os.path.join(out_dir,f\"{self.ID}.pkl\"))\n",
    "        \n",
    "def generate_embeddings(basename=\"163_A1a\",\n",
    "                        analysis_type=\"tumor\",\n",
    "                       gpu_id=0):\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    models={k:f\"models/{k}_map_cnn.pth\" for k in ['macro','tumor']}\n",
    "    num_classes=dict(macro=4,tumor=2)\n",
    "    train_model(model_save_loc=models[analysis_type],extract_embeddings=True,num_classes=num_classes[analysis_type],predict=True,embedding_out_dir=\"cnn_embeddings/\",custom_dataset=CustomDataset(patch_info_file,npy_file,generate_transformers(224,256)['test']),gpu_id=gpu_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_embeddings(basename=\"163_A1a\",\n",
    "#                         analysis_type=\"tumor\",\n",
    "#                        gpu_id=0)\n",
    "# generate_embeddings(basename=\"163_A1a\",\n",
    "#                         analysis_type=\"macro\",\n",
    "#                        gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, numpy as np, pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sps\n",
    "from torch_geometric.utils import subgraph, add_remaining_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from collections import Counter\n",
    "from torch_geometric.data import Data \n",
    "\n",
    "def create_graph_data(basename=\"163_A1a\",\n",
    "                      analysis_type=\"tumor\",\n",
    "                      radius=256,\n",
    "                      min_component_size=600):\n",
    "    embeddings=torch.load(f\"cnn_embeddings/{basename}_{analysis_type}_map.pkl\")\n",
    "    xy=torch.tensor(embeddings['patch_info'][['x','y']].values).float().cuda()\n",
    "    X=torch.tensor(embeddings['embeddings'])\n",
    "    G=radius_graph(xy, r=radius*np.sqrt(2), batch=None, loop=True)\n",
    "    G=G.detach().cpu()\n",
    "    G=add_remaining_self_loops(G)[0]\n",
    "    xy=xy.detach().cpu()\n",
    "    datasets=[]\n",
    "    edges=G.detach().cpu().numpy().astype(int)\n",
    "    n_components,components=list(sps.csgraph.connected_components(sps.coo_matrix((np.ones_like(edges[0]),(edges[0],edges[1])))))\n",
    "    comp_count=Counter(components)\n",
    "    components=torch.LongTensor(components)\n",
    "    for i in range(n_components):\n",
    "        if comp_count[i]>=min_component_size:\n",
    "            G_new=subgraph(components==i,G,relabel_nodes=True)[0]\n",
    "            xy_new=xy[components==i]\n",
    "            X_new=X[components==i]\n",
    "            np.random.seed(42)\n",
    "            idx=np.arange(X_new.shape[0])\n",
    "            idx2=np.arange(X_new.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            train_idx,val_idx,test_idx=torch.tensor(np.isin(idx2,idx[:int(0.8*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.8*len(idx)):int(0.9*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.9*len(idx)):]))\n",
    "            dataset=Data(x=X_new, edge_index=G_new, y_new=torch.ones(len(X_new)), edge_attr=None, pos=xy_new)\n",
    "            dataset.train_mask=train_idx\n",
    "            dataset.val_mask=val_idx\n",
    "            dataset.test_mask=test_idx\n",
    "            dataset.id=basename\n",
    "            dataset.component=i\n",
    "            datasets.append(dataset)\n",
    "    pickle.dump(datasets,open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_graph_data(basename=\"163_A1a\",\n",
    "#                       analysis_type=\"tumor\",\n",
    "#                       radius=256,\n",
    "#                       min_component_size=600)\n",
    "# create_graph_data(basename=\"163_A1a\",\n",
    "#                       analysis_type=\"macro\",\n",
    "#                       radius=256,\n",
    "#                       min_component_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, pickle, numpy as np, pandas as pd, torch.nn as nn\n",
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse, dropout_adj, to_networkx\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False, p=0.05, p2=0.1):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "        self.gcn.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.gcn.dropout = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for i,conv in enumerate(self.gcn.convs):\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if i+1<len(self.gcn.convs):\n",
    "                x=F.relu(x)\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = self.gcn.fc(F.relu(x))#F.softmax()\n",
    "        return x,y\n",
    "    \n",
    "def predict(basename=\"163_A1a\",\n",
    "            analysis_type=\"tumor\",\n",
    "            gpu_id=0):\n",
    "    hidden_topology=dict(tumor=[32]*3,macro=[32]*3)\n",
    "    num_classes=dict(macro=4,tumor=2)\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    dataset=pickle.load(open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'rb'))\n",
    "    model=GCNNet(dataset[0].x.shape[1],num_classes[analysis_type],hidden_topology=hidden_topology[analysis_type],p=0.,p2=0.)\n",
    "    model=model.cuda()\n",
    "    model.load_state_dict(torch.load(os.path.join(\"models\",f\"{analysis_type}_map_gnn.pth\"),map_location=f\"cuda:{gpu_id}\" if gpu_id>=0 else \"cpu\"))\n",
    "    dataloader=TG_DataLoader(dataset,shuffle=False,batch_size=1)\n",
    "    model.eval()\n",
    "    feature_extractor=GCNFeatures(model,bayes=False).cuda()\n",
    "    graphs=[]\n",
    "    for i,data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            graph = to_networkx(data).to_undirected()\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            xy=data.pos.numpy()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            preds=feature_extractor(x,edge_index)\n",
    "            z,y_pred=preds[0].detach().cpu().numpy(),preds[1].detach().cpu().numpy()\n",
    "            graphs.append(dict(G=graph,xy=xy,z=z,y_pred=y_pred,slide=data.id,component=data.component))\n",
    "    torch.save(graphs,os.path.join(\"gnn_results\",f\"{basename}_{analysis_type}_map.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(basename=\"163_A1a\",\n",
    "#             analysis_type=\"tumor\",\n",
    "#             gpu_id=0)\n",
    "# predict(basename=\"163_A1a\",\n",
    "#             analysis_type=\"macro\",\n",
    "#             gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclei prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch, pandas as pd, numpy as np\n",
    "import pickle\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "from tqdm import trange\n",
    "\n",
    "class WSI_Dataset(Dataset):\n",
    "    def __init__(self, patches, transform):\n",
    "        self.patches=patches\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.length=len(self.patches)\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        X=self.transform(self.to_pil(self.patches[idx]))\n",
    "        return X,torch.zeros(X.shape[-2:]).unsqueeze(0).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "def predict_nuclei(basename=\"163_A1a\",\n",
    "                   gpu_id=0):\n",
    "    analysis_type=\"tumor\"\n",
    "    patch_size=256\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    patches=np.load(npy_file)\n",
    "    custom_dataset=WSI_Dataset(patches,generate_transformers(256,256)['test'])\n",
    "    Y_seg=train_model(inputs_dir='inputs',\n",
    "                    architecture='resnet50',\n",
    "                    batch_size=512,\n",
    "                    num_classes=2,\n",
    "                    predict=True,\n",
    "                    model_save_loc=\"models/nuclei.pth\",\n",
    "                    predictions_save_path='tmp_test.pkl',\n",
    "                    predict_set='custom',\n",
    "                    verbose=False,\n",
    "                    class_balance=False,\n",
    "                    gpu_id=gpu_id,\n",
    "                    tensor_dataset=False,\n",
    "                    semantic_segmentation=True,\n",
    "                    custom_dataset=custom_dataset,\n",
    "                    save_predictions=False)['pred']\n",
    "\n",
    "    xy=pd.read_pickle(patch_info_file)[['x','y']].values\n",
    "    img_shape=np.load(f\"inputs/{basename}.npy\",mmap_mode=\"r\").shape[:-1]\n",
    "    pred_mask=np.zeros(img_shape)\n",
    "    for i in trange(Y_seg.shape[0]):\n",
    "        x,y=xy[i]\n",
    "        pred_mask[x:x+patch_size,y:y+patch_size]=Y_seg[i].argmax(0)\n",
    "    pred_mask=pred_mask.astype(bool)\n",
    "    np.save(f\"nuclei_results/{basename}.npy\",pred_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_nuclei(basename=\"163_A1a\",\n",
    "#                    gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate Mapper graphs macro+tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_cluster import nearest\n",
    "import sys, os, torch, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "sys.path.insert(0,os.path.abspath(\"./dgm/\"))\n",
    "from dgm.dgm import DGM\n",
    "from umap import UMAP\n",
    "import pickle\n",
    "\n",
    "classes_=['dermis', 'epidermis', 'hole', 'subcutaneous tissue']\n",
    "\n",
    "def relabel_tumor(graph_tumor,graph_macro):\n",
    "    le=LabelEncoder().fit(classes_)\n",
    "    re_idx=nearest(torch.tensor(graph_tumor['xy']), torch.tensor(graph_macro['xy'])).numpy()\n",
    "    unassigned=(graph_tumor['xy']-graph_macro['xy'][re_idx]).sum()!=0\n",
    "    macro_pred=graph_macro['y_pred'].argmax(1)\n",
    "    tumor_pred=graph_tumor['y_pred'].argmax(1)\n",
    "    benign=tumor_pred==0\n",
    "    tumor_pred=tumor_pred.astype('str')\n",
    "    tumor_pred[benign]=le.inverse_transform(macro_pred[re_idx][benign])\n",
    "    tumor_pred[~benign]='tumor'\n",
    "    tumor_pred[unassigned]='unassigned'\n",
    "    graph_tumor['annotation']=tumor_pred\n",
    "    return graph_tumor\n",
    "\n",
    "def construct_mapper(graph):\n",
    "    z=UMAP(n_components=2,random_state=42).fit_transform(graph['z'])\n",
    "    return dict(out_res=DGM(num_intervals=2,overlap=0.01,min_component_size=100,eps=0.1, sdgm=True).fit_transform(graph['G'], z),graph=graph)\n",
    "\n",
    "def get_interaction(out_graph,y_orig,res,lb=None,plot=False,le=None):\n",
    "    if not isinstance(lb,type(None)):\n",
    "        y_orig=lb.transform(y_orig)\n",
    "    node_makeup={}# only if predict\n",
    "    for node in out_graph.nodes():\n",
    "        nodes=res['mnode_to_nodes'][node]\n",
    "        node_makeup[node]=y_orig[np.array(list(nodes))].mean(0)\n",
    "    edges = out_graph.edges()\n",
    "    edge_weight=res['edge_weight']\n",
    "    weights = np.array([edge_weight[(min(u, v), max(u, v))] for u, v in edges], dtype=np.float32)\n",
    "    edgelist=list(edges)\n",
    "    A=np.zeros((len(lb.classes_),len(lb.classes_)))\n",
    "    for i in range(len(edgelist)):\n",
    "        send=node_makeup[edgelist[i][0]]\n",
    "        receive=node_makeup[edgelist[i][1]]\n",
    "        a=np.outer(send,receive)\n",
    "        a=(a+a.T)/2.*weights[i]\n",
    "        A+=a\n",
    "    invasion_mat=pd.DataFrame(A,columns=le.inverse_transform(np.arange(len(lb.classes_))),index=le.inverse_transform(np.arange(len(lb.classes_))))\n",
    "    return invasion_mat\n",
    "\n",
    "def calc_hole_vals(dgm_result,weights={'dermis':3,'epidermis':2,'subcutaneous tissue':1}):\n",
    "    y_pred=dgm_result['graph']['y_pred'].argmax(1)\n",
    "    out_graph,res=dgm_result['out_res']\n",
    "    le=LabelEncoder().fit(classes_)\n",
    "    area_hole=(le.inverse_transform(y_pred)=='hole').mean()\n",
    "    hole_share=get_interaction(out_graph,y_pred,res,lb=LabelBinarizer().fit(np.arange(len(le.classes_))),le=le)['hole']\n",
    "    hole_share=hole_share.loc[hole_share.index!='hole']\n",
    "    hole_share=pd.DataFrame(hole_share).reset_index()\n",
    "    hole_share2=hole_share.set_index('index')\n",
    "    hole_share2['weight']=pd.Series(weights)\n",
    "    hole_share2['importance']=hole_share2['weight']*hole_share2['hole']\n",
    "    return hole_share2\n",
    "\n",
    "def calc_tumor_vals(dgm_result,weights={'dermis':1,'epidermis':1,'subcutaneous tissue':1,'hole':1}):\n",
    "    out_graph,res=dgm_result['out_res']\n",
    "    le=LabelEncoder().fit(dgm_result['graph']['annotation'])\n",
    "    y=le.transform(dgm_result['graph']['annotation'])\n",
    "    tumor_share=get_interaction(out_graph,y,res,lb=LabelBinarizer().fit(np.arange(len(le.classes_))),le=le)['tumor']\n",
    "    tumor_share=tumor_share.loc[~tumor_share.index.isin(['tumor','unassigned'])]\n",
    "    tumor_share=pd.DataFrame(tumor_share).reset_index()\n",
    "    tumor_share2=tumor_share.set_index('index')\n",
    "    tumor_share2['weight']=pd.Series(weights)\n",
    "    tumor_share2['importance']=tumor_share2['weight']*tumor_share2['tumor']\n",
    "    return tumor_share2\n",
    "\n",
    "def generate_quality_scores(basename):\n",
    "    graphs={k:torch.load(os.path.join(\"gnn_results\",f\"{basename}_{k}_map.pkl\")) for k in ['tumor','macro']}\n",
    "    graphs['tumor']=[relabel_tumor(graph_tumor,graph_macro) for graph_tumor,graph_macro in zip(graphs['tumor'],graphs['macro'])]\n",
    "\n",
    "    mapper_graphs=dict()\n",
    "    for k in ['tumor','macro']:\n",
    "        mapper_graphs[k]=[construct_mapper(graph) for graph in graphs[k]]\n",
    "\n",
    "    scoring_fn=dict(tumor=calc_tumor_vals,macro=calc_hole_vals)\n",
    "    quality_score=dict()\n",
    "\n",
    "    for k in mapper_graphs:\n",
    "        quality_score[k]=pd.concat([scoring_fn[k](dgm_result)['importance'] for dgm_result in mapper_graphs[k]],axis=1)\n",
    "        quality_score[k].columns=[f'section_{i}' for i in range(1,len(quality_score[k].columns)+1)]\n",
    "\n",
    "    pickle.dump(quality_score,open(f'quality_scores/{basename}.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ink prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology as morph\n",
    "from scipy.ndimage import binary_opening, binary_dilation, label as scilabel\n",
    "from skimage import filters, measure\n",
    "from skimage.morphology import disk\n",
    "import numpy as np, pandas as pd, copy\n",
    "import sys,os,cv2\n",
    "from itertools import product\n",
    "sys.path.insert(0,os.path.abspath('.'))\n",
    "from filters import filter_red_pen, filter_blue_pen, filter_green_pen\n",
    "\n",
    "def filter_yellow(img): # https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/\n",
    "    img_hsv=cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    return cv2.inRange(img_hsv,(10, 30, 30), (30, 255, 255))\n",
    "\n",
    "ink_fn=dict(red=filter_red_pen,\n",
    "           blue=filter_blue_pen,\n",
    "           green=filter_green_pen,\n",
    "           yellow=filter_yellow)\n",
    "\n",
    "ink_min_size=dict(red=100,\n",
    "           blue=30,\n",
    "           green=30,\n",
    "           yellow=1000)\n",
    "\n",
    "colors=dict(red=np.array([255,0,0]),\n",
    "           blue=np.array([0,0,255]),\n",
    "           green=np.array([0,255,0]),\n",
    "           yellow=np.array([255,255,0]))\n",
    "\n",
    "def tune_mask(mask,edges,min_size=30):\n",
    "    mask=(binary_dilation(mask,disk(3,bool),iterations=5) & edges)\n",
    "    mask=binary_opening(mask,disk(3,bool),iterations=1)\n",
    "    return morph.remove_small_objects(mask, min_size=min_size, connectivity = 2, in_place=True)>0\n",
    "\n",
    "def filter_tune(img,color,edges):\n",
    "    return tune_mask(~ink_fn[color](img),edges,min_size=ink_min_size[color])\n",
    "\n",
    "def get_edges(mask):\n",
    "    edges=filters.sobel(mask)>0\n",
    "    edges = binary_dilation(edges,disk(30,bool))\n",
    "    return edges\n",
    "\n",
    "def detect_inks(basename=\"163_A1a\",\n",
    "                compression=8):\n",
    "    img,mask=np.load(f\"inputs/{basename}.npy\"),np.load(f\"masks/{basename}_macro_map.npy\")\n",
    "    img=cv2.resize(img,None,fx=1/compression,fy=1/compression)\n",
    "    mask=cv2.resize(mask.astype(int),None,fx=1/compression,fy=1/compression,interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "    labels,n_objects=scilabel(mask)\n",
    "    edges=get_edges(mask)\n",
    "    pen_masks={k:filter_tune(img,k,edges) for k in ink_fn}\n",
    "\n",
    "    for k in ['green','blue','red','yellow']:\n",
    "        img[pen_masks[k],:]=colors[k]\n",
    "\n",
    "    coords_df=pd.DataFrame(index=list(ink_fn.keys())+[\"center_mass\"],columns=np.arange(1,n_objects+1))\n",
    "    for color,obj in product(coords_df.index[:-1],coords_df.columns):\n",
    "        coords_df.loc[color,obj]=np.vstack(np.where((labels==obj) & (pen_masks[color]))).T*compression\n",
    "    for obj in coords_df.columns:\n",
    "        coords_df.loc[\"center_mass\",obj]=np.vstack(np.where(labels==obj)).T.mean(0)*compression\n",
    "\n",
    "    coords_df.to_pickle(f\"detected_inks/{basename}.pkl\")\n",
    "    np.save(f\"detected_inks/{basename}_thumbnail.npy\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow_series(basename):\n",
    "    print(f\"{basename} preprocessing\")\n",
    "    preprocess(basename=basename,\n",
    "               threshold=0.05,\n",
    "               patch_size=256)\n",
    "    \n",
    "    for k in ['tumor','macro']:\n",
    "        print(f\"{basename} {k} embedding\")\n",
    "        generate_embeddings(basename=basename,\n",
    "                            analysis_type=k,\n",
    "                           gpu_id=0)\n",
    "\n",
    "        print(f\"{basename} {k} build graph\")\n",
    "        create_graph_data(basename=basename,\n",
    "                          analysis_type=k,\n",
    "                          radius=256,\n",
    "                          min_component_size=600)\n",
    "        \n",
    "        print(f\"{basename} {k} gnn predict\")\n",
    "        predict(basename=basename,\n",
    "                analysis_type=k,\n",
    "                gpu_id=0)\n",
    "\n",
    "    print(f\"{basename} quality assessment\")\n",
    "    generate_quality_scores(basename)\n",
    "    \n",
    "    print(f\"{basename} ink detection\")\n",
    "    detect_inks(basename=basename,\n",
    "                compression=8)\n",
    "    \n",
    "    print(f\"{basename} nuclei detection\")\n",
    "    predict_nuclei(basename=basename,\n",
    "                   gpu_id=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "def run_series(patient=\"163_A1\"):\n",
    "    for f in glob.glob(f\"inputs/{patient}*.npy\"):\n",
    "        run_workflow_series(os.path.basename(f).replace(\".npy\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blend predicted scores\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize / cut sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,pickle, numpy as np\n",
    "\n",
    "def dump_results(patient=\"163_A1\",scheme=\"2/1\"):\n",
    "    n_sections,n_blocks_per_section=np.array(scheme.split(\"/\")).astype(int)\n",
    "    images=sorted(glob.glob(f\"inputs/{patient}*.npy\"))\n",
    "    masks=sorted(glob.glob(f\"inputs/{patient}*.npy\"))\n",
    "    gnn_results=sorted(glob.glob(f\"gnn_results/{patient}*.pkl\"))\n",
    "    quality_scores=sorted(glob.glob(f\"quality_scores/{patient}*.pkl\"))\n",
    "    ink_results=sorted(glob.glob(f\"detected_inks/{patient}*.pkl\"))\n",
    "    nuclei_results=sorted(glob.glob(f\"nuclei_results/{patient}*.npy\"))\n",
    "\n",
    "    pickle.dump(dict(n_sections=n_sections,\n",
    "                    n_blocks_per_section=n_blocks_per_section,\n",
    "                    images=images,\n",
    "                    masks=masks,\n",
    "                    gnn_results=gnn_results,\n",
    "                    quality_scores=quality_scores,\n",
    "                    ink_results=ink_results,\n",
    "                    nuclei_results=nuclei_results),open(f'results/{patient}.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alignment parameters\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply slide level alignment parameters to each exported image + mask\n",
    "# original, macro, tumor, nuclei\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality score from adjacent sections after extracting mapper scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "patient=\"163_A1\"\n",
    "\n",
    "results=pd.read_pickle(f\"results/{patient}.pkl\")\n",
    "quality_score={k:dict(enumerate([pd.concat([pd.read_pickle(f)[k].iloc[:,section::results['n_blocks_per_section']].fillna(0) for f in results['quality_scores']],axis=1) for section in range(results['n_blocks_per_section'])])) for k in ['tumor','macro']}\n",
    "# score per block per section ADD\n",
    "# average across blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dash to call prelim openseadragons; update with fine-tuned heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more here; turn each cell into python script after testing\n",
    "# arcticai package\n",
    "# airflow scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "def filter_mask(mask): # fill holes here (2 masks output) and ensure only have top X sections; find largest sections; do later\n",
    "    macro_mask=fill_holes(mask)\n",
    "    return mask, macro_mask\n",
    "\n",
    "# holes for certain analysis type; rip from pathflow and lower dependencies\n",
    "# preprocess\n",
    "# analysis_type=\"\"\n",
    "# turn into custom dataset for pathpretrain eval\n",
    "# predict(hidden_topology=[32]*3)\n",
    "# predict_nuclei()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
