{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 940cbe4] workflow update\n",
      " Committer: Joshua J. Levy <f003k8w@gv01.hpcc.dartmouth.edu>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly:\n",
      "\n",
      "    git config --global user.name \"Your Name\"\n",
      "    git config --global user.email you@example.com\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 1 file changed, 1922 insertions(+), 1922 deletions(-)\n",
      " rewrite workflow/0_workflow_components_practice.ipynb (99%)\n",
      "Counting objects: 4, done.\n",
      "Delta compression using up to 32 threads.\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 24.32 KiB | 2.43 MiB/s, done.\n",
      "Total 4 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# ! ln ../bcc/inputs/163_A1*.npy inputs\n",
    "# ! ln ../../prototype_validation_study/gnn_models/macro_gnn_model.pth models/macro_map_gnn.pth\n",
    "# ! ln ../../prototype_validation_study/gnn_models/tumor_gnn_model.pth models/tumor_map_gnn.pth\n",
    "# ! ln ../../prototype_validation_study/updated_checkpoints_cnn_macro_v2/24.epoch.checkpoint.pth models/macro_map_cnn.pth\n",
    "# ! ln ../../prototype_validation_study/updated_checkpoints_cnn_tumor/30.epoch.checkpoint.pth models/tumor_map_cnn.pth\n",
    "# ! cp -al ../bcc/web_test/openseadragon dzi_files\n",
    "# torch.save(GCNNet(2048, 4, [32]*3).state_dict(),\"models/macro_map_gnn.pth\")\n",
    "# ! pip uninstall pathpretrain -y && pip install git+https://github.com/jlevy44/PathPretrain\n",
    "# OLD\n",
    "# ! ln ../bcc/gnn_models/4.model.pth models/tumor_map_gnn.pth\n",
    "# ! ln ../bcc/pretrain_model.pth models/tumor_map_cnn.pth\n",
    "# ! ln ../fat_dermis_epi_sq_model/v2/checkpoints_tissue_seg/104.checkpoint.pth models/macro_map_cnn.pth\n",
    "# ! ln ../nuclei_pipeline/seg_model/27.checkpoint.pth models/nuclei.pth\n",
    "! cd ../ArcticAI_Prototype/ && git add * */* && git commit -a -m \"workflow update\" && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nonechucks may not work properly with this version of PyTorch (1.6.0). It has only been tested on PyTorch versions 1.0, 1.1, and 1.2\n"
     ]
    }
   ],
   "source": [
    "import os, tqdm\n",
    "import numpy as np, pandas as pd\n",
    "from pathflowai.utils import generate_tissue_mask\n",
    "from itertools import product\n",
    "from scipy.ndimage.morphology import binary_fill_holes as fill_holes\n",
    "\n",
    "def preprocess(basename=\"163_A1a\",\n",
    "               threshold=0.05,\n",
    "               patch_size=256):\n",
    "    \n",
    "    image=f\"inputs/{basename}.npy\"\n",
    "    basename=os.path.basename(image).replace('.npy','')\n",
    "    image=np.load(image)\n",
    "    \n",
    "    masks=dict()\n",
    "    masks['tumor_map']=generate_tissue_mask(image,\n",
    "                             compression=10,\n",
    "                             otsu=False,\n",
    "                             threshold=240,\n",
    "                             connectivity=8,\n",
    "                             kernel=5,\n",
    "                             min_object_size=100000,\n",
    "                             return_convex_hull=False,\n",
    "                             keep_holes=False,\n",
    "                             max_hole_size=6000,\n",
    "                             gray_before_close=True,\n",
    "                             blur_size=51) \n",
    "    x_max,y_max=masks['tumor_map'].shape\n",
    "    masks['macro_map']=fill_holes(masks['tumor_map'])\n",
    "    \n",
    "    patch_info=dict()\n",
    "    for k in masks:\n",
    "        patch_info[k]=pd.DataFrame([[basename,x,y,patch_size,\"0\"] for x,y in tqdm.tqdm(list(product(range(0,x_max-patch_size,patch_size),range(0,y_max-patch_size,patch_size))))],columns=['ID','x','y','patch_size','annotation'])\n",
    "        patches=np.stack([image[x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())])                   \n",
    "        include_patches=np.stack([masks[k][x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())]).mean((1,2))>=threshold\n",
    "\n",
    "        np.save(f\"masks/{basename}_{k}.npy\",masks[k])\n",
    "        np.save(f\"patches/{basename}_{k}.npy\",patches[include_patches]) \n",
    "        patch_info[k].iloc[include_patches].to_pickle(f\"patches/{basename}_{k}.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(basename=\"163_A1a\",\n",
    "#                threshold=0.05,\n",
    "#                patch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model\n",
    "import os, torch, tqdm, pandas as pd, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # load using saved patches and mask file\n",
    "    def __init__(self, patch_info, npy_file, transform):\n",
    "        self.X=np.load(npy_file)\n",
    "        self.patch_info=pd.read_pickle(patch_info)\n",
    "        self.xy=self.patch_info[['x','y']].values\n",
    "        self.patch_size=self.patch_info['patch_size'].iloc[0]\n",
    "        self.length=self.patch_info.shape[0]\n",
    "        self.transform=transform\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.ID=os.path.basename(npy_file).replace(\".npy\",\"\")\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        x,y=self.xy[i]\n",
    "        return self.transform(self.to_pil(self.X[i]))#[x:x+patch_size,y:y+patch_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def embed(self,model,batch_size,out_dir):\n",
    "        Z=[]\n",
    "        dataloader=DataLoader(self,batch_size=batch_size,shuffle=False)\n",
    "        n_batches=len(self)//batch_size\n",
    "        with torch.no_grad():\n",
    "            for i,X in tqdm.tqdm(enumerate(dataloader),total=n_batches):\n",
    "                if torch.cuda.is_available(): X=X.cuda()\n",
    "                z=model(X).detach().cpu().numpy()\n",
    "                Z.append(z)\n",
    "        Z=np.vstack(Z)\n",
    "        torch.save(dict(embeddings=Z,patch_info=self.patch_info),os.path.join(out_dir,f\"{self.ID}.pkl\"))\n",
    "        \n",
    "def generate_embeddings(basename=\"163_A1a\",\n",
    "                        analysis_type=\"tumor\",\n",
    "                       gpu_id=0):\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    models={k:f\"models/{k}_map_cnn.pth\" for k in ['macro','tumor']}\n",
    "    num_classes=dict(macro=4,tumor=3)\n",
    "    train_model(model_save_loc=models[analysis_type],extract_embeddings=True,num_classes=num_classes[analysis_type],predict=True,embedding_out_dir=\"cnn_embeddings/\",custom_dataset=CustomDataset(patch_info_file,npy_file,generate_transformers(224,256)['test']),gpu_id=gpu_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_embeddings(basename=\"163_A1a\",\n",
    "#                         analysis_type=\"tumor\",\n",
    "#                        gpu_id=0)\n",
    "# generate_embeddings(basename=\"163_A1a\",\n",
    "#                         analysis_type=\"macro\",\n",
    "#                        gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, numpy as np, pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sps\n",
    "from torch_geometric.utils import subgraph, add_remaining_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from collections import Counter\n",
    "from torch_geometric.data import Data \n",
    "\n",
    "def create_graph_data(basename=\"163_A1a\",\n",
    "                      analysis_type=\"tumor\",\n",
    "                      radius=256,\n",
    "                      min_component_size=600):\n",
    "    embeddings=torch.load(f\"cnn_embeddings/{basename}_{analysis_type}_map.pkl\")\n",
    "    xy=torch.tensor(embeddings['patch_info'][['x','y']].values).float().cuda()\n",
    "    X=torch.tensor(embeddings['embeddings'])\n",
    "    G=radius_graph(xy, r=radius*np.sqrt(2), batch=None, loop=True)\n",
    "    G=G.detach().cpu()\n",
    "    G=add_remaining_self_loops(G)[0]\n",
    "    xy=xy.detach().cpu()\n",
    "    datasets=[]\n",
    "    edges=G.detach().cpu().numpy().astype(int)\n",
    "    n_components,components=list(sps.csgraph.connected_components(sps.coo_matrix((np.ones_like(edges[0]),(edges[0],edges[1])))))\n",
    "    comp_count=Counter(components)\n",
    "    components=torch.LongTensor(components)\n",
    "    for i in range(n_components):\n",
    "        if comp_count[i]>=min_component_size:\n",
    "            G_new=subgraph(components==i,G,relabel_nodes=True)[0]\n",
    "            xy_new=xy[components==i]\n",
    "            X_new=X[components==i]\n",
    "            np.random.seed(42)\n",
    "            idx=np.arange(X_new.shape[0])\n",
    "            idx2=np.arange(X_new.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            train_idx,val_idx,test_idx=torch.tensor(np.isin(idx2,idx[:int(0.8*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.8*len(idx)):int(0.9*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.9*len(idx)):]))\n",
    "            dataset=Data(x=X_new, edge_index=G_new, y_new=torch.ones(len(X_new)), edge_attr=None, pos=xy_new)\n",
    "            dataset.train_mask=train_idx\n",
    "            dataset.val_mask=val_idx\n",
    "            dataset.test_mask=test_idx\n",
    "            dataset.id=basename\n",
    "            dataset.component=i\n",
    "            datasets.append(dataset)\n",
    "    pickle.dump(datasets,open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_graph_data(basename=\"163_A1a\",\n",
    "#                       analysis_type=\"tumor\",\n",
    "#                       radius=256,\n",
    "#                       min_component_size=600)\n",
    "# create_graph_data(basename=\"163_A1a\",\n",
    "#                       analysis_type=\"macro\",\n",
    "#                       radius=256,\n",
    "#                       min_component_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, pickle, numpy as np, pandas as pd, torch.nn as nn\n",
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse, dropout_adj, to_networkx\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False, p=0.05, p2=0.1):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "        self.gcn.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.gcn.dropout = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for i,conv in enumerate(self.gcn.convs):\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if i+1<len(self.gcn.convs):\n",
    "                x=F.relu(x)\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = self.gcn.fc(F.relu(x))#F.softmax()\n",
    "        return x,y\n",
    "    \n",
    "def predict(basename=\"163_A1a\",\n",
    "            analysis_type=\"tumor\",\n",
    "            gpu_id=0):\n",
    "    hidden_topology=dict(tumor=[32,64,64],macro=[32,64,64])#[32]*3\n",
    "    num_classes=dict(macro=4,tumor=3)\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    dataset=pickle.load(open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'rb'))\n",
    "    model=GCNNet(dataset[0].x.shape[1],num_classes[analysis_type],hidden_topology=hidden_topology[analysis_type],p=0.,p2=0.)\n",
    "    model=model.cuda()\n",
    "    model.load_state_dict(torch.load(os.path.join(\"models\",f\"{analysis_type}_map_gnn.pth\"),map_location=f\"cuda:{gpu_id}\" if gpu_id>=0 else \"cpu\"))\n",
    "    dataloader=TG_DataLoader(dataset,shuffle=False,batch_size=1)\n",
    "    model.eval()\n",
    "    feature_extractor=GCNFeatures(model,bayes=False).cuda()\n",
    "    graphs=[]\n",
    "    for i,data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            graph = to_networkx(data).to_undirected()\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            xy=data.pos.numpy()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            preds=feature_extractor(x,edge_index)\n",
    "            z,y_pred=preds[0].detach().cpu().numpy(),preds[1].detach().cpu().numpy()\n",
    "            graphs.append(dict(G=graph,xy=xy,z=z,y_pred=y_pred,slide=data.id,component=data.component))\n",
    "    torch.save(graphs,os.path.join(\"gnn_results\",f\"{basename}_{analysis_type}_map.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(basename=\"163_A1a\",\n",
    "#             analysis_type=\"tumor\",\n",
    "#             gpu_id=0)\n",
    "# predict(basename=\"163_A1a\",\n",
    "#             analysis_type=\"macro\",\n",
    "#             gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclei prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch, pandas as pd, numpy as np\n",
    "import pickle\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "from tqdm import trange\n",
    "\n",
    "class WSI_Dataset(Dataset):\n",
    "    def __init__(self, patches, transform):\n",
    "        self.patches=patches\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.length=len(self.patches)\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        X=self.transform(self.to_pil(self.patches[idx]))\n",
    "        return X,torch.zeros(X.shape[-2:]).unsqueeze(0).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "def predict_nuclei(basename=\"163_A1a\",\n",
    "                   gpu_id=0):\n",
    "    analysis_type=\"tumor\"\n",
    "    patch_size=256\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    patches=np.load(npy_file)\n",
    "    custom_dataset=WSI_Dataset(patches,generate_transformers(256,256)['test'])\n",
    "    Y_seg=train_model(inputs_dir='inputs',\n",
    "                    architecture='resnet50',\n",
    "                    batch_size=512,\n",
    "                    num_classes=2,\n",
    "                    predict=True,\n",
    "                    model_save_loc=\"models/nuclei.pth\",\n",
    "                    predictions_save_path='tmp_test.pkl',\n",
    "                    predict_set='custom',\n",
    "                    verbose=False,\n",
    "                    class_balance=False,\n",
    "                    gpu_id=gpu_id,\n",
    "                    tensor_dataset=False,\n",
    "                    semantic_segmentation=True,\n",
    "                    custom_dataset=custom_dataset,\n",
    "                    save_predictions=False)['pred']\n",
    "\n",
    "    xy=pd.read_pickle(patch_info_file)[['x','y']].values\n",
    "    img_shape=np.load(f\"inputs/{basename}.npy\",mmap_mode=\"r\").shape[:-1]\n",
    "    pred_mask=np.zeros(img_shape)\n",
    "    for i in trange(Y_seg.shape[0]):\n",
    "        x,y=xy[i]\n",
    "        pred_mask[x:x+patch_size,y:y+patch_size]=Y_seg[i].argmax(0)\n",
    "    pred_mask=pred_mask.astype(bool)\n",
    "    np.save(f\"nuclei_results/{basename}.npy\",pred_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_nuclei(basename=\"163_A1a\",\n",
    "#                    gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate Mapper graphs macro+tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_cluster import nearest\n",
    "import sys, os, torch, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "sys.path.insert(0,os.path.abspath(\"./dgm/\"))\n",
    "from dgm.dgm import DGM\n",
    "from umap import UMAP\n",
    "import pickle\n",
    "\n",
    "classes_=['dermis', 'epidermis', 'hole', 'subcutaneous tissue']\n",
    "\n",
    "def relabel_tumor(graph_tumor,graph_macro):\n",
    "    le=LabelEncoder().fit(classes_)\n",
    "    re_idx=nearest(torch.tensor(graph_tumor['xy']), torch.tensor(graph_macro['xy'])).numpy()\n",
    "    unassigned=(graph_tumor['xy']-graph_macro['xy'][re_idx]).sum()!=0\n",
    "    macro_pred=graph_macro['y_pred'].argmax(1)\n",
    "    tumor_pred=graph_tumor['y_pred'].argmax(1)\n",
    "    benign=tumor_pred!=2#0 <- former model\n",
    "    tumor_pred=tumor_pred.astype('str')\n",
    "    tumor_pred[benign]=le.inverse_transform(macro_pred[re_idx][benign])\n",
    "    tumor_pred[~benign]='tumor'\n",
    "    tumor_pred[unassigned]='unassigned'\n",
    "    graph_tumor['annotation']=tumor_pred\n",
    "    return graph_tumor\n",
    "\n",
    "def construct_mapper(graph):\n",
    "    z=UMAP(n_components=2,random_state=42).fit_transform(graph['z'])\n",
    "    return dict(out_res=DGM(num_intervals=2,overlap=0.01,min_component_size=100,eps=0.1, sdgm=True).fit_transform(graph['G'], z),graph=graph)\n",
    "\n",
    "def get_interaction(out_graph,y_orig,res,lb=None,plot=False,le=None):\n",
    "    if not isinstance(lb,type(None)):\n",
    "        y_orig=lb.transform(y_orig)\n",
    "    node_makeup={}# only if predict\n",
    "    for node in out_graph.nodes():\n",
    "        nodes=res['mnode_to_nodes'][node]\n",
    "        node_makeup[node]=y_orig[np.array(list(nodes))].mean(0)\n",
    "    edges = out_graph.edges()\n",
    "    edge_weight=res['edge_weight']\n",
    "    weights = np.array([edge_weight[(min(u, v), max(u, v))] for u, v in edges], dtype=np.float32)\n",
    "    edgelist=list(edges)\n",
    "    A=np.zeros((len(lb.classes_),len(lb.classes_)))\n",
    "    for i in range(len(edgelist)):\n",
    "        send=node_makeup[edgelist[i][0]]\n",
    "        receive=node_makeup[edgelist[i][1]]\n",
    "        a=np.outer(send,receive)\n",
    "        a=(a+a.T)/2.*weights[i]\n",
    "        A+=a\n",
    "    invasion_mat=pd.DataFrame(A,columns=le.inverse_transform(np.arange(len(lb.classes_))),index=le.inverse_transform(np.arange(len(lb.classes_))))\n",
    "    return invasion_mat\n",
    "\n",
    "def calc_hole_vals(dgm_result,weights={'dermis':1,'epidermis':1,'subcutaneous tissue':1}):\n",
    "    y_pred=dgm_result['graph']['y_pred'].argmax(1)\n",
    "    out_graph,res=dgm_result['out_res']\n",
    "    le=LabelEncoder().fit(classes_)\n",
    "    area_hole=(le.inverse_transform(y_pred)=='hole').mean()\n",
    "    hole_share=get_interaction(out_graph,y_pred,res,lb=LabelBinarizer().fit(np.arange(len(le.classes_))),le=le)['hole']\n",
    "    hole_share=hole_share.loc[hole_share.index!='hole']\n",
    "    hole_share=pd.DataFrame(hole_share).reset_index()\n",
    "    hole_share2=hole_share.set_index('index')\n",
    "    hole_share2['weight']=pd.Series(weights)\n",
    "    hole_share2['importance']=hole_share2['weight']*hole_share2['hole']\n",
    "    return hole_share2\n",
    "\n",
    "def calc_tumor_vals(dgm_result,weights={'dermis':1,'epidermis':1,'subcutaneous tissue':1,'hole':1}):\n",
    "    out_graph,res=dgm_result['out_res']\n",
    "    le=LabelEncoder().fit(dgm_result['graph']['annotation'])\n",
    "    y=le.transform(dgm_result['graph']['annotation'])\n",
    "    tumor_share=get_interaction(out_graph,y,res,lb=LabelBinarizer().fit(np.arange(len(le.classes_))),le=le)['tumor']\n",
    "    tumor_share=tumor_share.loc[~tumor_share.index.isin(['tumor','unassigned'])]\n",
    "    tumor_share=pd.DataFrame(tumor_share).reset_index()\n",
    "    tumor_share2=tumor_share.set_index('index')\n",
    "    tumor_share2['weight']=pd.Series(weights)\n",
    "    tumor_share2['importance']=tumor_share2['weight']*tumor_share2['tumor']\n",
    "    return tumor_share2\n",
    "\n",
    "def generate_quality_scores(basename):\n",
    "    graphs={k:torch.load(os.path.join(\"gnn_results\",f\"{basename}_{k}_map.pkl\")) for k in ['tumor','macro']}\n",
    "    graphs['tumor']=[relabel_tumor(graph_tumor,graph_macro) for graph_tumor,graph_macro in zip(graphs['tumor'],graphs['macro'])]\n",
    "\n",
    "    mapper_graphs=dict()\n",
    "    for k in ['tumor','macro']:\n",
    "        mapper_graphs[k]=[construct_mapper(graph) for graph in graphs[k]]\n",
    "\n",
    "    scoring_fn=dict(tumor=calc_tumor_vals,macro=calc_hole_vals)\n",
    "    quality_score=dict()\n",
    "\n",
    "    for k in mapper_graphs:\n",
    "        quality_score[k]=pd.concat([scoring_fn[k](dgm_result)['importance'] for dgm_result in mapper_graphs[k]],axis=1)\n",
    "        quality_score[k].columns=[f'section_{i}' for i in range(1,len(quality_score[k].columns)+1)]\n",
    "\n",
    "    pickle.dump(mapper_graphs,open(f'mapper_graphs/{basename}.pkl','wb'))\n",
    "    pickle.dump(quality_score,open(f'quality_scores/{basename}.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ink prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology as morph\n",
    "from scipy.ndimage import binary_opening, binary_dilation, label as scilabel\n",
    "from skimage import filters, measure\n",
    "from skimage.morphology import disk\n",
    "import numpy as np, pandas as pd, copy\n",
    "import sys,os,cv2\n",
    "from itertools import product\n",
    "sys.path.insert(0,os.path.abspath('.'))\n",
    "from filters import filter_red_pen, filter_blue_pen, filter_green_pen\n",
    "\n",
    "def filter_yellow(img): # https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/\n",
    "    img_hsv=cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    return cv2.inRange(img_hsv,(10, 30, 30), (30, 255, 255))\n",
    "\n",
    "ink_fn=dict(red=filter_red_pen,\n",
    "           blue=filter_blue_pen,\n",
    "           green=filter_green_pen,\n",
    "           yellow=filter_yellow)\n",
    "\n",
    "ink_min_size=dict(red=100,\n",
    "           blue=30,\n",
    "           green=30,\n",
    "           yellow=1000)\n",
    "\n",
    "colors=dict(red=np.array([255,0,0]),\n",
    "           blue=np.array([0,0,255]),\n",
    "           green=np.array([0,255,0]),\n",
    "           yellow=np.array([255,255,0]))\n",
    "\n",
    "def tune_mask(mask,edges,min_size=30):\n",
    "    mask=(binary_dilation(mask,disk(3,bool),iterations=5) & edges)\n",
    "    mask=binary_opening(mask,disk(3,bool),iterations=1)\n",
    "    return morph.remove_small_objects(mask, min_size=min_size, connectivity = 2, in_place=True)>0\n",
    "\n",
    "def filter_tune(img,color,edges):\n",
    "    return tune_mask(~ink_fn[color](img),edges,min_size=ink_min_size[color])\n",
    "\n",
    "def get_edges(mask):\n",
    "    edges=filters.sobel(mask)>0\n",
    "    edges = binary_dilation(edges,disk(30,bool))\n",
    "    return edges\n",
    "\n",
    "def detect_inks(basename=\"163_A1a\",\n",
    "                compression=8):\n",
    "    img,mask=np.load(f\"inputs/{basename}.npy\"),np.load(f\"masks/{basename}_macro_map.npy\")\n",
    "    img=cv2.resize(img,None,fx=1/compression,fy=1/compression)\n",
    "    mask=cv2.resize(mask.astype(int),None,fx=1/compression,fy=1/compression,interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "    labels,n_objects=scilabel(mask)\n",
    "    edges=get_edges(mask)\n",
    "    pen_masks={k:filter_tune(img,k,edges) for k in ink_fn}\n",
    "\n",
    "    for k in ['green','blue','red','yellow']:\n",
    "        img[pen_masks[k],:]=colors[k]\n",
    "\n",
    "    coords_df=pd.DataFrame(index=list(ink_fn.keys())+[\"center_mass\"],columns=np.arange(1,n_objects+1))\n",
    "    for color,obj in product(coords_df.index[:-1],coords_df.columns):\n",
    "        coords_df.loc[color,obj]=np.vstack(np.where((labels==obj) & (pen_masks[color]))).T*compression\n",
    "    for obj in coords_df.columns:\n",
    "        coords_df.loc[\"center_mass\",obj]=np.vstack(np.where(labels==obj)).T.mean(0)*compression\n",
    "\n",
    "    coords_df.to_pickle(f\"detected_inks/{basename}.pkl\")\n",
    "    np.save(f\"detected_inks/{basename}_thumbnail.npy\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow_series(basename):\n",
    "    print(f\"{basename} preprocessing\")\n",
    "    preprocess(basename=basename,\n",
    "               threshold=0.05,\n",
    "               patch_size=256)\n",
    "    \n",
    "    for k in ['tumor','macro']:\n",
    "        print(f\"{basename} {k} embedding\")\n",
    "        generate_embeddings(basename=basename,\n",
    "                            analysis_type=k,\n",
    "                           gpu_id=0)\n",
    "\n",
    "        print(f\"{basename} {k} build graph\")\n",
    "        create_graph_data(basename=basename,\n",
    "                          analysis_type=k,\n",
    "                          radius=256,\n",
    "                          min_component_size=600)\n",
    "        \n",
    "        print(f\"{basename} {k} gnn predict\")\n",
    "        predict(basename=basename,\n",
    "                analysis_type=k,\n",
    "                gpu_id=0)\n",
    "\n",
    "    print(f\"{basename} quality assessment\")\n",
    "    generate_quality_scores(basename)\n",
    "    \n",
    "    print(f\"{basename} ink detection\")\n",
    "    detect_inks(basename=basename,\n",
    "                compression=8)\n",
    "    \n",
    "    print(f\"{basename} nuclei detection\")\n",
    "    predict_nuclei(basename=basename,\n",
    "                   gpu_id=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "def run_series(patient=\"163_A1\"):\n",
    "    for f in glob.glob(f\"inputs/{patient}*.npy\"):\n",
    "        run_workflow_series(os.path.basename(f).replace(\".npy\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1b preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48184/48184 [00:00<00:00, 3049022.28it/s]\n",
      "100%|██████████| 48184/48184 [00:00<00:00, 2107671.99it/s]\n",
      "100%|██████████| 48184/48184 [00:00<00:00, 2151057.91it/s]\n",
      "100%|██████████| 48184/48184 [00:00<00:00, 1246251.31it/s]\n",
      "100%|██████████| 48184/48184 [00:00<00:00, 1978020.83it/s]\n",
      "100%|██████████| 48184/48184 [00:00<00:00, 2154084.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1b tumor embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "815it [00:41, 19.60it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1b tumor build graph\n",
      "163_A1b tumor gnn predict\n",
      "163_A1b macro embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "821it [00:41, 19.69it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1b macro build graph\n",
      "163_A1b macro gnn predict\n",
      "163_A1b quality assessment\n",
      "Mapper graph nodes 247\n",
      "Mapper graph edges 21\n",
      "Mapper graph nodes 216\n",
      "Mapper graph edges 16\n",
      "Mapper graph nodes 169\n",
      "Mapper graph edges 36\n",
      "Mapper graph nodes 155\n",
      "Mapper graph edges 27\n",
      "163_A1b ink detection\n",
      "163_A1b nuclei detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:29,  1.76s/it]                        \n",
      "100%|██████████| 26070/26070 [00:16<00:00, 1554.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1c preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48150/48150 [00:00<00:00, 2587849.02it/s]\n",
      "100%|██████████| 48150/48150 [00:00<00:00, 1900294.87it/s]\n",
      "100%|██████████| 48150/48150 [00:00<00:00, 1809104.28it/s]\n",
      "100%|██████████| 48150/48150 [00:00<00:00, 1823478.71it/s]\n",
      "100%|██████████| 48150/48150 [00:00<00:00, 1709924.29it/s]\n",
      "100%|██████████| 48150/48150 [00:00<00:00, 2134500.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1c tumor embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "783it [00:38, 20.29it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1c tumor build graph\n",
      "163_A1c tumor gnn predict\n",
      "163_A1c macro embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "789it [00:39, 20.19it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1c macro build graph\n",
      "163_A1c macro gnn predict\n",
      "163_A1c quality assessment\n",
      "Mapper graph nodes 181\n",
      "Mapper graph edges 64\n",
      "Mapper graph nodes 257\n",
      "Mapper graph edges 52\n",
      "Mapper graph nodes 158\n",
      "Mapper graph edges 19\n",
      "Mapper graph nodes 170\n",
      "Mapper graph edges 31\n",
      "163_A1c ink detection\n",
      "163_A1c nuclei detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [01:27,  1.78s/it]                        \n",
      "100%|██████████| 25039/25039 [00:16<00:00, 1490.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1a preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48924/48924 [00:00<00:00, 2470082.80it/s]\n",
      "100%|██████████| 48924/48924 [00:00<00:00, 2002167.32it/s]\n",
      "100%|██████████| 48924/48924 [00:00<00:00, 1878624.27it/s]\n",
      "100%|██████████| 48924/48924 [00:00<00:00, 1166418.24it/s]\n",
      "100%|██████████| 48924/48924 [00:00<00:00, 1225811.85it/s]\n",
      "100%|██████████| 48924/48924 [00:00<00:00, 2141269.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1a tumor embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "771it [00:38, 20.03it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1a tumor build graph\n",
      "163_A1a tumor gnn predict\n",
      "163_A1a macro embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "805it [00:40, 19.98it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163_A1a macro build graph\n",
      "163_A1a macro gnn predict\n",
      "163_A1a quality assessment\n",
      "Mapper graph nodes 191\n",
      "Mapper graph edges 28\n",
      "Mapper graph nodes 161\n",
      "Mapper graph edges 22\n",
      "Mapper graph nodes 153\n",
      "Mapper graph edges 53\n",
      "Mapper graph nodes 187\n",
      "Mapper graph edges 33\n",
      "163_A1a ink detection\n",
      "163_A1a nuclei detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [01:26,  1.76s/it]                        \n",
      "100%|██████████| 24661/24661 [00:16<00:00, 1475.31it/s]\n"
     ]
    }
   ],
   "source": [
    "run_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blend predicted scores\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize / cut sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,pickle, numpy as np\n",
    "\n",
    "def dump_results(patient=\"163_A1\",scheme=\"2/1\"):\n",
    "    n_sections_per_slide,n_blocks_per_section=np.array(scheme.split(\"/\")).astype(int)\n",
    "    images=sorted(glob.glob(f\"inputs/{patient}*.npy\"))\n",
    "    image_ids=np.vectorize(lambda x: os.path.basename(x).replace(\".npy\",\"\"))(images)\n",
    "    masks=sorted(glob.glob(f\"masks/{patient}*macro*.npy\"))\n",
    "    tumor_gnn_results=sorted(glob.glob(f\"gnn_results/{patient}*tumor*.pkl\"))\n",
    "    macro_gnn_results=sorted(glob.glob(f\"gnn_results/{patient}*macro*.pkl\"))\n",
    "    quality_scores=sorted(glob.glob(f\"quality_scores/{patient}*.pkl\"))\n",
    "    ink_results=sorted(glob.glob(f\"detected_inks/{patient}*.pkl\"))\n",
    "    nuclei_results=sorted(glob.glob(f\"nuclei_results/{patient}*.npy\"))\n",
    "\n",
    "    pickle.dump(dict(n_slides=len(images),\n",
    "                    image_ids=image_ids,\n",
    "                    n_sections_per_slide=n_sections_per_slide,\n",
    "                    n_blocks_per_section=n_blocks_per_section,\n",
    "                    images=images,\n",
    "                    masks=masks,\n",
    "                    tumor_gnn_results=tumor_gnn_results,\n",
    "                    macro_gnn_results=macro_gnn_results,\n",
    "                    quality_scores=quality_scores,\n",
    "                    ink_results=ink_results,\n",
    "                    nuclei_results=nuclei_results),open(f'results/{patient}.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi']=300\n",
    "matplotlib.rcParams['axes.grid'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from dgm.plotting import *\n",
    "import copy\n",
    "def display_results(out_graphs,res_,predict=False,custom_colors=[],s=1,img=None,alpha=None,scatter=True,scale=8,width_scale=20,node_scale=90,preds=None):\n",
    "    f = plt.figure(figsize=(15,15))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    \n",
    "    if not isinstance(img,type(None)): plt.imshow(np.transpose(img,(1,0,2)))\n",
    "    \n",
    "    for out_graph,res,pred in zip(out_graphs,res_,preds):\n",
    "        xy=pred[\"xy\"]\n",
    "        y_orig=pred[\"y\"] \n",
    "        y=copy.deepcopy(y_orig)\n",
    "        graph=out_graph\n",
    "        binary=False\n",
    "        node_color=res['mnode_to_color']; node_size=res['node_sizes']; edge_weight=res['edge_weight']\n",
    "        if custom_colors: node_color=custom_colors\n",
    "        node_list=res['node_list']; name='wsi'\n",
    "        cmap = cm.coolwarm\n",
    "        cmap = cm.get_cmap(cmap, 100)\n",
    "        plt.set_cmap(cmap)\n",
    "\n",
    "        \n",
    "\n",
    "        edges = graph.edges()\n",
    "        weights = np.array([edge_weight[(min(u, v), max(u, v))] for u, v in edges], dtype=np.float32)\n",
    "\n",
    "        width = weights * width_scale\n",
    "\n",
    "        node_size = np.sqrt(node_size) * node_scale\n",
    "        c=y.flatten()\n",
    "\n",
    "        pos = {}\n",
    "        for node in graph.nodes():\n",
    "            if len(res['mnode_to_nodes'][node])-1:\n",
    "                pos[node]=np.array([xy[i] for i in res['mnode_to_nodes'][node]]).mean(0)/scale\n",
    "            else:\n",
    "                pos[node]=xy[list(res['mnode_to_nodes'][node])[0]]/scale\n",
    "\n",
    "\n",
    "\n",
    "        if scatter: plt.scatter(xy[:,0]/scale,xy[:,1]/scale,c=c,alpha=alpha,s=s)\n",
    "        nx.draw(graph, pos=pos, node_color=node_color, width=width, node_size=node_size,\n",
    "                node_list=node_list, ax=ax, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.grid(b=None)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename=\"163_A1c\"\n",
    "img=np.load(f\"inputs/{basename}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression=4\n",
    "\n",
    "im=cv2.resize(img,None,fx=1/compression,fy=1/compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(im)\n",
    "# plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_graphs=pd.read_pickle(f\"mapper_graphs/{basename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k='macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mapper_graphs[k])):\n",
    "    mapper_graphs[k][i]['graph']['y']=mapper_graphs[k][i]['graph']['y_pred'].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graphs,res_,preds=[mapper_graphs[k][i]['out_res'][0] for i in range(len(mapper_graphs[k]))],[mapper_graphs[k][i]['out_res'][1] for i in range(len(mapper_graphs[k]))],[mapper_graphs[k][i]['graph'] for i in range(len(mapper_graphs[k]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_results(out_graphs,res_,alpha=0.2,s=20,img=im,preds=preds,scale=compression,node_scale=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alignment parameters\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply slide level alignment parameters to each exported image + mask\n",
    "# original, macro, tumor, nuclei\n",
    "# do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality score from adjacent sections after extracting mapper scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 663M\r\n",
      "drwxrwx---  2 f003k8w rc-VaickusL_slow-admin  204 Dec 18 19:56 .\r\n",
      "drwxrwx--- 19 f003k8w rc-VaickusL_slow-admin  612 Dec 26 04:48 ..\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin  26M Dec 18 19:11 163_A1a.pkl\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin 145M Dec 18 19:11 163_A1a_thumbnail.npy\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin  25M Dec 18 20:20 163_A1b.pkl\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin 142M Dec 18 20:21 163_A1b_thumbnail.npy\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin  24M Dec 18 20:25 163_A1c.pkl\r\n",
      "-rwxrwx---  1 f003k8w rc-VaickusL_slow-admin 142M Dec 18 20:25 163_A1c_thumbnail.npy\r\n"
     ]
    }
   ],
   "source": [
    "! ls detected_inks/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inks=pd.read_pickle(\"detected_inks/163_A1a.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>[[1616, 23520], [1624, 23504], [1624, 23512], ...</td>\n",
       "      <td>[[2224, 77376], [2224, 77384], [2224, 77392], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>[[1672, 25856], [1672, 25864], [1672, 25872], ...</td>\n",
       "      <td>[[2320, 72880], [2328, 72864], [2328, 72872], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>[[4744, 14264], [4744, 14272], [4752, 14264], ...</td>\n",
       "      <td>[[4680, 65232], [4680, 65240], [4688, 65232], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow</th>\n",
       "      <td>[[1496, 23176], [1496, 23184], [1496, 23192], ...</td>\n",
       "      <td>[[2216, 78080], [2216, 78088], [2216, 78096], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>center_mass</th>\n",
       "      <td>[18395.52081605944, 20204.459783962207]</td>\n",
       "      <td>[18463.00644006811, 65057.845274241554]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             1  \\\n",
       "red          [[1616, 23520], [1624, 23504], [1624, 23512], ...   \n",
       "blue         [[1672, 25856], [1672, 25864], [1672, 25872], ...   \n",
       "green        [[4744, 14264], [4744, 14272], [4752, 14264], ...   \n",
       "yellow       [[1496, 23176], [1496, 23184], [1496, 23192], ...   \n",
       "center_mass            [18395.52081605944, 20204.459783962207]   \n",
       "\n",
       "                                                             2  \n",
       "red          [[2224, 77376], [2224, 77384], [2224, 77392], ...  \n",
       "blue         [[2320, 72880], [2328, 72864], [2328, 72872], ...  \n",
       "green        [[4680, 65232], [4680, 65240], [4688, 65232], ...  \n",
       "yellow       [[2216, 78080], [2216, 78088], [2216, 78096], ...  \n",
       "center_mass            [18463.00644006811, 65057.845274241554]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd, numpy as np\n",
    "# from scipy.ndimage import label as scilabel\n",
    "# from skimage.measure import regionprops_table\n",
    "# import cv2, os, subprocess\n",
    "# from deepzoom import *\n",
    "# from deepzoom import _get_or_create_path,_get_files_path\n",
    "# from PIL import Image\n",
    "# import tqdm\n",
    "# import dask\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# from scipy.special import softmax\n",
    "# import torch\n",
    "# from sauth import SimpleHTTPAuthHandler, serve_http\n",
    "# from skimage.draw import circle\n",
    "# Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# colors=dict(red=np.array([255,0,0]),\n",
    "#            blue=np.array([0,0,255]),\n",
    "#            green=np.array([0,255,0]),\n",
    "#            yellow=np.array([255,255,0]))\n",
    "\n",
    "# class Numpy2DZI(ImageCreator):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         tile_size=254,\n",
    "#         tile_overlap=1,\n",
    "#         tile_format=\"jpg\",\n",
    "#         image_quality=0.8,\n",
    "#         resize_filter=None,\n",
    "#         copy_metadata=False,\n",
    "#         compression=1.\n",
    "#     ):\n",
    "#         super().__init__(tile_size,tile_overlap,tile_format,image_quality,resize_filter,copy_metadata)\n",
    "#         self.compression=compression\n",
    "        \n",
    "#     def create(self, source_arr, destination):\n",
    "#         # potentially have an option where dynamically softlink once deeper layer is made so slide is readily available, push to background process and write metadata for dash app to read  \n",
    "#         # speed up image saving with dask https://stackoverflow.com/questions/54615625/how-to-save-dask-array-as-png-files-slice-by-slice https://github.com/dask/dask-image/issues/110\n",
    "#         self.image = PIL.Image.fromarray(source_arr if self.compression==1 else cv2.resize(source_arr,None,fx=1/self.compression,fy=1/self.compression,interpolation=cv2.INTER_CUBIC))\n",
    "#         width, height = self.image.size\n",
    "#         self.descriptor = DeepZoomImageDescriptor(\n",
    "#             width=width,\n",
    "#             height=height,\n",
    "#             tile_size=self.tile_size,\n",
    "#             tile_overlap=self.tile_overlap,\n",
    "#             tile_format=self.tile_format,\n",
    "#         )\n",
    "#         image_files = _get_or_create_path(_get_files_path(destination))\n",
    "#         for level in tqdm.trange(self.descriptor.num_levels, desc='level'):\n",
    "#             level_dir = _get_or_create_path(os.path.join(image_files, str(level)))\n",
    "#             level_image = self.get_image(level)\n",
    "#             for (column, row) in tqdm.tqdm(self.tiles(level), desc='tiles'):\n",
    "#                 bounds = self.descriptor.get_tile_bounds(level, column, row)\n",
    "#                 tile = level_image.crop(bounds)\n",
    "#                 format = self.descriptor.tile_format\n",
    "#                 tile_path = os.path.join(level_dir, \"%s_%s.%s\" % (column, row, format))\n",
    "#                 tile_file = open(tile_path, \"wb\")\n",
    "#                 if self.descriptor.tile_format == \"jpg\":\n",
    "#                     jpeg_quality = int(self.image_quality * 100)\n",
    "#                     tile.save(tile_file, \"JPEG\", quality=jpeg_quality)\n",
    "#                 else:\n",
    "#                     tile.save(tile_file)\n",
    "#         self.descriptor.save(destination)\n",
    "#         return destination\n",
    "        \n",
    "# @dask.delayed\n",
    "# def write_dzi(img, out_dzi, compression=8):\n",
    "#     return Numpy2DZI(compression=compression).create(img,out_dzi)\n",
    "\n",
    "# def add_depth(x):\n",
    "#     x=x.sort_values(['slide_id',\"section_id\"])\n",
    "#     x['depth']=np.arange(1,len(x)+1)\n",
    "#     return x\n",
    "\n",
    "# def mask2label(mask,compression=8):\n",
    "#     mask_small=cv2.resize(mask.astype(int),None,fx=1/compression,fy=1/compression,interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "#     label=cv2.resize(scilabel(mask_small)[0],dsize=mask.shape[::-1],interpolation=cv2.INTER_NEAREST)\n",
    "#     return label\n",
    "\n",
    "# class Case:\n",
    "#     def __init__(self, patient=\"163_A1\"):\n",
    "#         self.launch_dir=os.path.abspath(\".\")\n",
    "#         self.patient=patient\n",
    "#         self.results=pd.read_pickle(f\"results/{patient}.pkl\")\n",
    "#         self.slide_metadata,self.section_metadata=self.add_metadata(self.results)\n",
    "#         self.n_slides=self.results['n_slides']\n",
    "#         self.slide_cache=self.slide_metadata.copy()\n",
    "#         self.section_cache=self.section_metadata.copy()\n",
    "#         self.slide_cache['label']=''\n",
    "#         self.slide_cache['tumor_map']=''\n",
    "#         self.slide_cache['macro_map']=''\n",
    "#         self.slide_cache['region_props']=''\n",
    "#         self.slide_inks={}\n",
    "#         self.section_metadata['quality']=''\n",
    "#         self.max_depth=self.section_metadata['depth'].max()\n",
    "#         self.n_blocks=self.section_metadata['block_id'].max()\n",
    "#         self.extraction_methods=dict(image=self.extract_section_image,\n",
    "#                                     tumor=self.extract_tumor_results,\n",
    "#                                     ink=self.extract_ink_results,\n",
    "#                                     nuclei=self.extract_nuclei_results)\n",
    "        \n",
    "        \n",
    "#     def add_metadata(self, results):\n",
    "#         slide_metadata=pd.DataFrame({k:results[k] for k in results if isinstance(results[k],list)}).reset_index().rename(columns=dict(index=\"slide_id\"))\n",
    "#         slide_metadata['slide_id']+=1\n",
    "#         section_metadata=dict(slide_id=[],\n",
    "#                              block_id=[],\n",
    "#                              section_id=[],\n",
    "#                              label_id=[])\n",
    "#         for slide in slide_metadata['slide_id'].values:\n",
    "#             section_metadata['label_id'].extend(np.arange(1,results['n_sections_per_slide']*results['n_blocks_per_section']+1))\n",
    "#             section_metadata['slide_id'].extend([slide]*(results['n_sections_per_slide']*results['n_blocks_per_section']))\n",
    "#             section_metadata['block_id'].extend(np.arange(1,results['n_blocks_per_section']+1).tolist()*results['n_sections_per_slide'])\n",
    "#             for i in range(1,results['n_sections_per_slide']+1): section_metadata['section_id'].extend([i]*results['n_blocks_per_section'])\n",
    "#         section_metadata=pd.DataFrame(section_metadata)\n",
    "#         section_metadata['id']=np.arange(len(section_metadata))\n",
    "#         section_metadata=pd.DataFrame(section_metadata.groupby(\"block_id\").apply(add_depth)).reset_index(drop=True).sort_values(['id'])\n",
    "#         return slide_metadata,section_metadata\n",
    "            \n",
    "    \n",
    "#     def compute_quality(self, importance_regions={'dermis':3.,'epidermis':1.,'subcutaneous tissue':2.},\n",
    "#                                 importance_tumor=4.,\n",
    "#                                 distance_weight=0.7,\n",
    "#                                 baseline_region=1.):\n",
    "        \n",
    "#         quality_scores=self.slide_metadata['quality_scores'].map(pd.read_pickle)\n",
    "\n",
    "#         tumor_quality_scores=pd.concat([quality_scores[i]['tumor'] for i in range(self.n_slides)],axis=1).fillna(0).T.reset_index(drop=True)\n",
    "#         macro_quality_scores=pd.concat([quality_scores[i]['macro'] for i in range(self.n_slides)],axis=1).fillna(0).T.reset_index(drop=True)\n",
    "\n",
    "#         self.section_metadata['quality']=np.nan\n",
    "#         for block_id in self.section_metadata['block_id'].unique():\n",
    "#             idx=self.section_metadata['block_id'].values==block_id\n",
    "#             section_metadata_ids=self.section_metadata['id'].loc[idx]\n",
    "#             macro_qual,tumor_qual=macro_quality_scores.loc[idx],tumor_quality_scores.loc[idx]\n",
    "#             for i in range(len(macro_qual)):\n",
    "#                 macro_qual['distance_weight']=distance_weight**(np.abs(macro_qual.index-i))\n",
    "#                 tumor_qual['distance_weight']=distance_weight**(np.abs(tumor_qual.index-i))\n",
    "#                 quality_score=pd.concat([importance_regions[region]*(macro_qual[region]*(baseline_region+tumor_qual[region]*tumor_qual[\"distance_weight\"])) for region in importance_regions]+[importance_tumor*tumor_qual['hole']*tumor_qual[\"distance_weight\"]],axis=1)\n",
    "#                 quality_score.columns=list(importance_regions)+['hole']\n",
    "#                 self.section_metadata.loc[self.section_metadata['id']==section_metadata_ids[i],'quality']=quality_score.values.sum()\n",
    "\n",
    "#     def load_slide(self, slide, compression=8):\n",
    "#         slide_loc=self.get_slide_loc(slide)\n",
    "#         if self.slide_cache.loc[slide_loc,['images','masks','label']].map(lambda x: isinstance(x,str)).sum()>0:\n",
    "#             self.slide_cache.loc[slide_loc,['images','masks']]=self.slide_metadata.loc[slide_loc,['images','masks']].map(np.load)\n",
    "#             self.slide_cache.loc[slide_loc,'label']=[mask2label(self.slide_cache.loc[slide_loc,'masks'],compression)]\n",
    "#             self.slide_cache.loc[slide_loc,'region_props']=[regionprops_table(self.slide_cache.loc[slide_loc,'label'], properties=['bbox'])] \n",
    "#         image,mask,label=self.slide_cache.loc[slide_loc,['images','masks','label']].tolist()\n",
    "#         return image,mask,label\n",
    "    \n",
    "#     def get_slide_loc(self, slide):\n",
    "#         return self.slide_metadata.index[np.where(self.slide_metadata['slide_id']==slide)[0][0]]\n",
    "    \n",
    "#     def get_section_bbox(self, slide_id, label_id):\n",
    "#         slide_loc=self.get_slide_loc(slide_id)\n",
    "#         bbox=pd.DataFrame(self.slide_cache.loc[slide_loc,'region_props'])\n",
    "#         bbox.columns=['xmin','ymin','xmax','ymax']\n",
    "#         bbox.index+=1\n",
    "#         xmin,ymin,xmax,ymax=bbox.loc[label_id]\n",
    "#         return xmin,ymin,xmax,ymax\n",
    "    \n",
    "#     def load_nuclei(self, slide):\n",
    "#         slide_loc=self.get_slide_loc(slide)\n",
    "#         if not isinstance(self.slide_cache.loc[slide_loc,'nuclei_results'],np.ndarray):\n",
    "#             nuclei_result=np.load(self.slide_cache.loc[slide_loc,'nuclei_results'])\n",
    "#             self.slide_cache.loc[slide_loc,'nuclei_results']=[nuclei_result]\n",
    "#         else: nuclei_result=self.slide_cache.loc[slide_loc,'nuclei_results']\n",
    "#         return nuclei_result \n",
    "    \n",
    "#     def load_tumor_map(self, slide, alpha=0.1, patch_size=256, low_res=True):\n",
    "#         assert low_res, \"High resolution label propagation completed, but not available as an option yet\"\n",
    "#         slide_loc=self.get_slide_loc(slide)\n",
    "#         if not isinstance(self.slide_cache.loc[slide_loc,'tumor_gnn_results'],np.ndarray):\n",
    "#             graphs=torch.load(self.slide_cache.loc[slide_loc,'tumor_gnn_results'])\n",
    "#             xy=np.vstack([graph['xy'] for graph in graphs]).astype(int)\n",
    "#             y_pred=softmax(np.vstack([graph['y_pred'] for graph in graphs]),1)[:,1].reshape(-1,1)\n",
    "#             img_=self.load_slide(slide)[0].copy()\n",
    "#             one_square=np.ones((patch_size,patch_size)).astype(np.float)*255\n",
    "#             for x,y,pred in tqdm.tqdm(np.hstack([xy,y_pred]).tolist(), desc='tumor'):\n",
    "#                 x,y=map(int,[x,y])\n",
    "#                 img_[x:x+patch_size,y:y+patch_size]=alpha*cv2.applyColorMap(np.uint8(pred*one_square), cv2.COLORMAP_JET)+(1-alpha)*img_[x:x+patch_size,y:y+patch_size]\n",
    "#             self.slide_cache.loc[slide_loc,'tumor_gnn_results']=[img_]\n",
    "#         else: img_=self.slide_cache.loc[slide_loc,'tumor_gnn_results']\n",
    "#         return img_\n",
    "    \n",
    "#     def load_ink(self, slide):\n",
    "#         slide_loc=self.get_slide_loc(slide)\n",
    "#         ink_file=self.slide_cache.loc[slide_loc,'ink_results']\n",
    "#         self.slide_inks.update({slide_loc:self.slide_inks.get(slide_loc,pd.read_pickle(ink_file))})\n",
    "#         return self.slide_inks[slide_loc]\n",
    "\n",
    "#     def extract_section_image(self, depth, block_id, compression=8):\n",
    "#         section=self.section_metadata.loc[(self.section_metadata['depth']==depth) & (self.section_metadata['block_id']==block_id)]\n",
    "#         label_id,slide_id=section.loc[:,['label_id','slide_id']].values.flatten()\n",
    "#         image,mask,label=self.load_slide(slide_id,compression)\n",
    "#         xmin,ymin,xmax,ymax=self.get_section_bbox(slide_id,label_id)\n",
    "        \n",
    "#         img=image[xmin:xmax,ymin:ymax].copy()\n",
    "#         img[label[xmin:xmax,ymin:ymax]!=label_id]=255\n",
    "#         return img\n",
    "    \n",
    "#     def extract_tumor_results(self, depth, block_id, alpha=0.3, patch_size=256, low_res=True, compression=8):\n",
    "#         section=self.section_metadata.loc[(self.section_metadata['depth']==depth) & (self.section_metadata['block_id']==block_id)]\n",
    "#         label_id,slide_id=section.loc[:,['label_id','slide_id']].values.flatten()\n",
    "#         _,mask,label=self.load_slide(slide_id,compression)\n",
    "#         xmin,ymin,xmax,ymax=self.get_section_bbox(slide_id,label_id)\n",
    "#         tumor_map=self.load_tumor_map(slide_id, alpha=alpha, patch_size=patch_size, low_res=low_res)\n",
    "                \n",
    "#         img=tumor_map[xmin:xmax,ymin:ymax].copy()\n",
    "#         img[label[xmin:xmax,ymin:ymax]!=label_id]=255\n",
    "#         return img\n",
    "    \n",
    "#     def extract_nuclei_results(self, depth, block_id, compression=8):\n",
    "#         section=self.section_metadata.loc[(self.section_metadata['depth']==depth) & (self.section_metadata['block_id']==block_id)]\n",
    "#         label_id,slide_id=section.loc[:,['label_id','slide_id']].values.flatten()\n",
    "#         image,mask,label=self.load_slide(slide_id,compression)\n",
    "#         xmin,ymin,xmax,ymax=self.get_section_bbox(slide_id,label_id)\n",
    "        \n",
    "#         nuclei=self.load_nuclei(slide_id)\n",
    "        \n",
    "#         img=image[xmin:xmax,ymin:ymax].copy()\n",
    "#         nuc_mask=nuclei[xmin:xmax,ymin:ymax].copy()\n",
    "#         img[nuc_mask,:]=[255,0,0]\n",
    "#         img[label[xmin:xmax,ymin:ymax]!=label_id]=255\n",
    "#         return img\n",
    "    \n",
    "#     def extract_ink_results(self, depth, block_id, circle_size=200, compression=8):\n",
    "#         section=self.section_metadata.loc[(self.section_metadata['depth']==depth) & (self.section_metadata['block_id']==block_id)]\n",
    "#         label_id,slide_id=section.loc[:,['label_id','slide_id']].values.flatten()\n",
    "#         image,mask,label=self.load_slide(slide_id,compression)\n",
    "#         xmin,ymin,xmax,ymax=self.get_section_bbox(slide_id,label_id)\n",
    "#         xy_min=np.array([xmin,ymin])\n",
    "#         ink=self.load_ink(slide_id)[label_id]\n",
    "#         ink=ink.map(lambda x: (x-xy_min).astype(int))\n",
    "#         img=image[xmin:xmax,ymin:ymax].copy()\n",
    "#         max_size=np.array(img.shape[:2])\n",
    "#         for k in colors:\n",
    "#             if k!=\"center_mass\": \n",
    "#                 ink_k=ink.loc[k]\n",
    "#                 remove=(~np.any((ink_k-max_size)>0,axis=1))\n",
    "#                 ink_k=ink_k[remove]\n",
    "#                 img[ink_k[:,0],ink_k[:,1],:]=colors[k]\n",
    "#             else: \n",
    "#                 xx,yy=circle(*(ink.loc[k].astype(int).tolist()), circle_size)\n",
    "#                 img[xx,yy,:]=[0,0,0]\n",
    "#         img[label[xmin:xmax,ymin:ymax]!=label_id]=255\n",
    "#         return img    \n",
    "    \n",
    "#     def write_dzi(self, img, out_dzi, compression=8):\n",
    "#         Numpy2DZI(compression=compression).create(img,out_dzi)\n",
    "        \n",
    "#     def write_dzi_parallel(self, img_dzi_dict, compression=8, scheduler='processes'):\n",
    "#         written_dzis=[]\n",
    "#         for out_dzi, img in img_dzi_dict.items():\n",
    "#             written_dzis.append(write_dzi(img, out_dzi, compression))\n",
    "#         with ProgressBar():\n",
    "#             written_dzis=dask.compute(*written_dzis, scheduler=scheduler)\n",
    "#         return written_dzis\n",
    "    \n",
    "#     def launch_server(self, username='username', password='password', port=5554):\n",
    "#         self.reset_dir()\n",
    "#         SimpleHTTPAuthHandler.username = username\n",
    "#         SimpleHTTPAuthHandler.password = password\n",
    "#         serve_http(ip='localhost', port=port, https=False,\n",
    "#                start_dir='dzi_files', handler_class=SimpleHTTPAuthHandler)\n",
    "        \n",
    "#     def reset_dir(self):\n",
    "#         os.chdir(self.launch_dir)\n",
    "    \n",
    "#     def visualize_dzi(self, dzis):\n",
    "#         self.reset_dir()\n",
    "#         replace_txt='\",\"'.join(list(map(os.path.basename,dzis)))\n",
    "#         with open(\"osd_template.html\") as f_in, open('dzi_files/index.html','w') as f_out:\n",
    "#             f_out.write(f_in.read().replace(\"REPLACE\",replace_txt).replace(\"BASENAME\",self.patient))\n",
    "            \n",
    "#     def extract2dzi(self, image_type='image', scheduler='single-threaded'):\n",
    "#         assert image_type in ['image','nuclei','tumor','ink']\n",
    "#         dzi_files=[]\n",
    "#         imgs={}\n",
    "#         for block in tqdm.trange(1,self.n_blocks+1, desc='block'):\n",
    "#             for depth in tqdm.trange(1,self.max_depth+1, desc='depth'):\n",
    "#                 section_info=[self.patient,depth,block,image_type]\n",
    "#                 dzi_file=f\"dzi_files/{'_'.join(list(map(str,section_info)))}.dzi\"\n",
    "#                 dzi_files.append(dzi_file)\n",
    "#                 if not os.path.exists(dzi_file):\n",
    "#                     imgs[dzi_file]=self.extraction_methods[image_type](depth,block)\n",
    "#         if len(imgs)>0: self.write_dzi_parallel(imgs,scheduler=scheduler)\n",
    "#         return dzi_files\n",
    "        \n",
    "            \n",
    "#     # add dzi check and database to reference and view dzi of particular layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case=Case()\n",
    "# dzis=dict(image=[],nuclei=[],tumor=[],ink=[])\n",
    "# for k in dzis:\n",
    "#     dzis[k]=case.extract2dzi(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "block:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "depth: 100%|██████████| 6/6 [00:00<00:00, 277.30it/s]\n",
      "block: 100%|██████████| 1/1 [00:00<00:00, 40.75it/s]\n",
      "block:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "depth: 100%|██████████| 6/6 [00:00<00:00, 836.27it/s]\n",
      "block: 100%|██████████| 1/1 [00:00<00:00, 102.18it/s]\n",
      "block:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "depth: 100%|██████████| 6/6 [00:00<00:00, 88.52it/s]\n",
      "block: 100%|██████████| 1/1 [00:00<00:00, 14.11it/s]\n",
      "block:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "depth: 100%|██████████| 6/6 [00:00<00:00, 162.63it/s]\n",
      "block: 100%|██████████| 1/1 [00:00<00:00, 24.85it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging inks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue_ink=case.slide_inks[0][1].loc['blue'].copy()\n",
    "case.slide_inks[0][1].loc['blue']=blue_ink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "blue\n",
      "green\n",
      "yellow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b2a24030bd0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "i,j=2,2\n",
    "G=kneighbors_graph(np.vstack([case.slide_inks[j][i].loc['green'],case.slide_inks[j][i].loc['blue']]),n_neighbors=10)[-case.slide_inks[j][i].loc['blue'].shape[0]:,:case.slide_inks[j][i].loc['green'].shape[0]]\n",
    "case.slide_inks[0][i].loc['blue']=case.slide_inks[j][i].loc['blue'][~G.sum(1).astype(bool).A.flatten()]\n",
    "plt.figure()\n",
    "for k in case.slide_inks[j][i].index[:-1]:\n",
    "    print(k)\n",
    "    if k!='yellow':\n",
    "        ink_k=case.slide_inks[j][i].loc[k]\n",
    "        components=connected_components(radius_neighbors_graph(ink_k,256), directed=False, return_labels=True)[1]\n",
    "        ink_k=ink_k[components==np.argmax(np.bincount(components))]\n",
    "        plt.scatter(*ink_k.T.tolist(),c=k,s=0.2)\n",
    "        plt.scatter(*np.median(ink_k,0).tolist(),c=k,s=25)\n",
    "plt.scatter(*case.slide_inks[j][i].loc['center_mass'].tolist(),s=200,color='black')\n",
    "#kneighbors_graph(ink_k,n_neighbors=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case.visualize_dzi(dzis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case.reset_dir()\n",
    "# case.launch_server(port=5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case.reset_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good enough for web app\n",
    "# build around dzi files, then add tissue quality scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix, then adopt https://dash-gallery.plotly.host/dash-financial-report/\n",
    "# from jupyter_dash import JupyterDash\n",
    "# import dash_bootstrap_components as dbc\n",
    "# import dash_core_components as dcc\n",
    "# import dash_html_components as html\n",
    "# import dash_table\n",
    "# from dash.dependencies import Output, Input\n",
    "\n",
    "\n",
    "# app = JupyterDash(__name__,external_stylesheets=[dbc.themes.COSMO],assets_folder=os.path.abspath(\"dzi_files\"))#'active_learning/round_1/unlabelled/',requests_pathname_prefix='/proxy/{}/'.format(port))\n",
    "\n",
    "# app.config.suppress_callback_exceptions = False\n",
    "\n",
    "# server = app.server\n",
    "\n",
    "# app.layout = html.Div([\n",
    "#     html.Iframe(id='openseadragon',\n",
    "#                 src=os.path.abspath(\"dzi_files/index.html\"))\n",
    "#     ])\n",
    "\n",
    "# port=5554\n",
    "# # app._terminate_server_for_port(\"localhost\", port)\n",
    "# app.run_server(debug=True,mode=\"external\",host='localhost',port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://localhost:5554/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dash to call prelim openseadragons; update with fine-tuned heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more here; turn each cell into python script after testing\n",
    "# arcticai package\n",
    "# airflow scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "# class Section:\n",
    "#     def __init__(self, image='', depth=1, block_type=1):\n",
    "        \n",
    "    \n",
    "\n",
    "# patient=\"163_A1\"\n",
    "\n",
    "# results=pd.read_pickle(f\"results/{patient}.pkl\")\n",
    "# quality_score={k:dict(enumerate([pd.concat([pd.read_pickle(f)[k].iloc[:,section::results['n_blocks_per_section']].fillna(0) for f in results['quality_scores']],axis=1) for section in range(results['n_blocks_per_section'])])) for k in ['tumor','macro']}\n",
    "# score per block per section ADD\n",
    "# average across blocks?\n",
    "def filter_mask(mask): # fill holes here (2 masks output) and ensure only have top X sections; find largest sections; do later\n",
    "    macro_mask=fill_holes(mask)\n",
    "    return mask, macro_mask\n",
    "\n",
    "# holes for certain analysis type; rip from pathflow and lower dependencies\n",
    "# preprocess\n",
    "# analysis_type=\"\"\n",
    "# turn into custom dataset for pathpretrain eval\n",
    "# predict(hidden_topology=[32]*3)\n",
    "# predict_nuclei()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
