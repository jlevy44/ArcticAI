{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ln ../bcc/inputs/163_A1*.npy inputs\n",
    "# ! ln ../bcc/gnn_models/4.model.pth models/tumor_map_gnn.pth\n",
    "# ! ln ../bcc/pretrain_model.pth models/tumor_map_cnn.pth\n",
    "# ! ln ../fat_dermis_epi_sq_model/v2/checkpoints_tissue_seg/104.checkpoint.pth models/macro_map_cnn.pth\n",
    "# ! ln ../nuclei_pipeline/seg_model/27.checkpoint.pth models/nuclei.pth\n",
    "# ! pip uninstall pathpretrain -y && pip install git+https://github.com/jlevy44/PathPretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm\n",
    "import numpy as np, pandas as pd\n",
    "from pathflowai.utils import generate_tissue_mask\n",
    "from itertools import product\n",
    "from scipy.ndimage.morphology import binary_fill_holes as fill_holes\n",
    "\n",
    "def preprocess(basename=\"163_A1a\",\n",
    "               threshold=0.05,\n",
    "               patch_size=256):\n",
    "    \n",
    "    image=f\"inputs/{basename}.npy\"\n",
    "    basename=os.path.basename(image).replace('.npy','')\n",
    "    image=np.load(image)\n",
    "    \n",
    "    masks=dict()\n",
    "    masks['tumor_map']=generate_tissue_mask(image,\n",
    "                             compression=10,\n",
    "                             otsu=False,\n",
    "                             threshold=240,\n",
    "                             connectivity=8,\n",
    "                             kernel=5,\n",
    "                             min_object_size=100000,\n",
    "                             return_convex_hull=False,\n",
    "                             keep_holes=False,\n",
    "                             max_hole_size=6000,\n",
    "                             gray_before_close=True,\n",
    "                             blur_size=51) \n",
    "    x_max,y_max=mask.shape\n",
    "    masks['macro_map']=fill_holes(masks['tumor_map'])\n",
    "    \n",
    "    patch_info=dict()\n",
    "    for k in masks:\n",
    "        patch_info[k]=pd.DataFrame([[basename,x,y,patch_size,\"0\"] for x,y in tqdm.tqdm(list(product(range(0,x_max-patch_size,patch_size),range(0,y_max-patch_size,patch_size))))],columns=['ID','x','y','patch_size','annotation'])\n",
    "        patches=np.stack([image[x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())])                   \n",
    "        include_patches=np.stack([masks[k][x:x+patch_size,y:y+patch_size] for x,y in tqdm.tqdm(patch_info[k][['x','y']].values.tolist())]).mean((1,2))>=threshold\n",
    "\n",
    "        np.save(f\"masks/{basename}_{k}.npy\",masks[k])\n",
    "        np.save(f\"patches/{basename}_{k}.npy\",patches[include_patches]) \n",
    "        patch_info[k].iloc[include_patches].to_pickle(f\"patches/{basename}_{k}.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict model\n",
    "import os, torch, tqdm, pandas as pd, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # load using saved patches and mask file\n",
    "    def __init__(self, patch_info, npy_file, transform):\n",
    "        self.X=np.load(npy_file)\n",
    "        self.patch_info=pd.read_pickle(patch_info)\n",
    "        self.xy=self.patch_info[['x','y']].values\n",
    "        self.patch_size=self.patch_info['patch_size'].iloc[0]\n",
    "        self.length=self.patch_info.shape[0]\n",
    "        self.transform=transform\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.ID=os.path.basename(npy_file).replace(\".npy\",\"\")\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        x,y=self.xy[i]\n",
    "        return self.transform(self.to_pil(self.X[i]))#[x:x+patch_size,y:y+patch_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def embed(self,model,batch_size,out_dir):\n",
    "        Z=[]\n",
    "        dataloader=DataLoader(self,batch_size=batch_size,shuffle=False)\n",
    "        n_batches=len(self)//batch_size\n",
    "        with torch.no_grad():\n",
    "            for i,X in tqdm.tqdm(enumerate(dataloader),total=n_batches):\n",
    "                if torch.cuda.is_available(): X=X.cuda()\n",
    "                z=model(X).detach().cpu().numpy()\n",
    "                Z.append(z)\n",
    "        Z=np.vstack(Z)\n",
    "        torch.save(dict(embeddings=Z,patch_info=self.patch_info),os.path.join(out_dir,f\"{self.ID}.pkl\"))\n",
    "        \n",
    "def generate_embeddings(basename=\"163_A1a\",\n",
    "                        analysis_type=\"tumor\",\n",
    "                       gpu_id=0):\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    models={k:f\"models/{k}_map_cnn.pth\" for k in ['macro','tumor']}\n",
    "    num_classes=dict(macro=4,tumor=2)\n",
    "    train_model(model_save_loc=models[analysis_type],extract_embeddings=True,num_classes=num_classes[analysis_type],predict=True,embedding_out_dir=\"cnn_embeddings/\",custom_dataset=CustomDataset(patch_info_file,npy_file,generate_transformers(224,256)['test']),gpu_id=gpu_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, numpy as np, pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sps\n",
    "from torch_geometric.utils import subgraph, add_remaining_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from collections import Counter\n",
    "from torch_geometric.data import Data \n",
    "\n",
    "def create_graph_data(basename=\"163_A1a\",\n",
    "                      analysis_type=\"tumor\",\n",
    "                      radius=256,\n",
    "                      min_component_size=600):\n",
    "    embeddings=torch.load(f\"cnn_embeddings/{basename}_{analysis_type}_map.pkl\")\n",
    "    xy=torch.tensor(embeddings['patch_info'][['x','y']].values).float().cuda()\n",
    "    X=torch.tensor(embeddings['embeddings'])\n",
    "    G=radius_graph(xy, r=radius*np.sqrt(2), batch=None, loop=True)\n",
    "    G=G.detach().cpu()\n",
    "    G=add_remaining_self_loops(G)[0]\n",
    "    xy=xy.detach().cpu()\n",
    "    datasets=[]\n",
    "    edges=G.detach().cpu().numpy().astype(int)\n",
    "    n_components,components=list(sps.csgraph.connected_components(sps.coo_matrix((np.ones_like(edges[0]),(edges[0],edges[1])))))\n",
    "    comp_count=Counter(components)\n",
    "    components=torch.LongTensor(components)\n",
    "    for i in range(n_components):\n",
    "        if comp_count[i]>=min_component_size:\n",
    "            G_new=subgraph(components==i,G,relabel_nodes=True)[0]\n",
    "            xy_new=xy[components==i]\n",
    "            X_new=X[components==i]\n",
    "            np.random.seed(42)\n",
    "            idx=np.arange(X_new.shape[0])\n",
    "            idx2=np.arange(X_new.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            train_idx,val_idx,test_idx=torch.tensor(np.isin(idx2,idx[:int(0.8*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.8*len(idx)):int(0.9*len(idx))])),torch.tensor(np.isin(idx2,idx[int(0.9*len(idx)):]))\n",
    "            dataset=Data(x=X_new, edge_index=G_new, y_new=torch.ones(len(X_new)), edge_attr=None, pos=xy_new)\n",
    "            dataset.train_mask=train_idx\n",
    "            dataset.val_mask=val_idx\n",
    "            dataset.test_mask=test_idx\n",
    "            dataset.id=basename\n",
    "            dataset.component=i\n",
    "            datasets.append(dataset)\n",
    "    pickle.dump(datasets,open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, pickle, numpy as np, pandas as pd, torch.nn as nn\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse, dropout_adj, to_networkx\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False, p=0.05, p2=0.1):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "        self.gcn.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.gcn.dropout = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for i,conv in enumerate(self.gcn.convs):\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if i+1<len(self.gcn.convs):\n",
    "                x=F.relu(x)\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = self.gcn.fc(F.relu(x))#F.softmax()\n",
    "        return x,y\n",
    "    \n",
    "def predict(basename=\"163_A1a\",\n",
    "            analysis_type=\"tumor\",\n",
    "            gpu_id=0):\n",
    "    hidden_topology=dict(tumor=[32]*3,macro=[32]*3)\n",
    "    num_classes=dict(macro=4,tumor=2)\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    dataset=pickle.load(open(os.path.join('graph_datasets',f\"{basename}_{analysis_type}_map.pkl\"),'rb'))\n",
    "    model=GCNNet(dataset[0].x.shape[1],num_classes[analysis_type],hidden_topology=hidden_topology[analysis_type],p=0.,p2=0.)\n",
    "    model=model.cuda()\n",
    "    model.load_state_dict(torch.load(os.path.join(\"models\",f\"{analysis_type}_map_gnn.pth\"),map_location=f\"cuda:{gpu_id}\" if gpu_id>=0 else \"cpu\"))\n",
    "    dataloader=DataLoader(dataset,shuffle=False,batch_size=1)\n",
    "    model.eval()\n",
    "    feature_extractor=GCNFeatures(model,bayes=False).cuda()\n",
    "    graphs=[]\n",
    "    for i,data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            graph = to_networkx(data).to_undirected()\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            xy=data.pos.numpy()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            preds=feature_extractor(x,edge_index)\n",
    "            z,y_pred=preds[0].detach().cpu().numpy(),preds[1].detach().cpu().numpy()\n",
    "            graphs.append(dict(G=graph,xy=xy,z=z,y_pred=y_pred,slide=data.id,component=data.component))\n",
    "    torch.save(graphs,os.path.join(\"gnn_results\",f\"{basename}_{analysis_type}_map.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclei prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch, pandas as pd, numpy as np\n",
    "import pickle\n",
    "from pathpretrain.train_model import train_model, generate_transformers, generate_kornia_transforms\n",
    "from tqdm import trange\n",
    "\n",
    "class WSI_Dataset(Dataset):\n",
    "    def __init__(self, patches, transform):\n",
    "        self.patches=patches\n",
    "        self.to_pil=lambda x: Image.fromarray(x)\n",
    "        self.length=len(self.patches)\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        X=self.transform(self.to_pil(self.patches[idx]))\n",
    "        return X,torch.zeros(X.shape[-2:]).unsqueeze(0).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "def predict_nuclei(basename=\"163_A1a\",\n",
    "                   analysis_type=\"tumor\",\n",
    "                   gpu_id=0):\n",
    "    patch_size=256\n",
    "    patch_info_file,npy_file=f\"patches/{basename}_{analysis_type}_map.pkl\",f\"patches/{basename}_{analysis_type}_map.npy\"\n",
    "    patches=np.load(npy_file)\n",
    "    custom_dataset=WSI_Dataset(patches,generate_transformers(256,256)['test'])\n",
    "    Y_seg=train_model(inputs_dir='inputs',\n",
    "                    architecture='resnet50',\n",
    "                    batch_size=512,\n",
    "                    num_classes=2,\n",
    "                    predict=True,\n",
    "                    model_save_loc=\"models/nuclei.pth\",\n",
    "                    predictions_save_path='tmp_test.pkl',\n",
    "                    predict_set='custom',\n",
    "                    verbose=False,\n",
    "                    class_balance=False,\n",
    "                    gpu_id=gpu_id,\n",
    "                    tensor_dataset=False,\n",
    "                    semantic_segmentation=True,\n",
    "                    custom_dataset=custom_dataset,\n",
    "                    save_predictions=False)['pred']\n",
    "\n",
    "    xy=pd.read_pickle(patch_info_file)[['x','y']].values\n",
    "    img_shape=np.load(f\"inputs/{basename}.npy\",mmap_mode=\"r\").shape[:-1]\n",
    "    pred_mask=np.zeros(img_shape)\n",
    "    for i in trange(Y_seg.shape[0]):\n",
    "        x,y=xy[i]\n",
    "        pred_mask[x:x+patch_size,y:y+patch_size]=Y_seg[i]\n",
    "    pred_mask=pred_mask.astype(bool)\n",
    "    np.save(f\"nuclei_results/{basename}.npy\",pred_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_5_pretrain_bcc.ipynb\t\t\t\t   8_thumbnail_inspect.ipynb\r\n",
      "0_generate_thumbnails_edit_graph_dataset.ipynb\t   LICENSE\r\n",
      "1_generate_graph_data.ipynb\t\t\t   README.md\r\n",
      "2_cv_splits.ipynb\t\t\t\t   command_history\r\n",
      "3_5_extract_features.ipynb\t\t\t   complete_old\r\n",
      "3_7_interpolate_label_expansion_apply_image.ipynb  extract_features.py\r\n",
      "3_9_prepare_dzi.ipynb\t\t\t\t   filters.py\r\n",
      "3_train_gnn_model.ipynb\t\t\t\t   ink\r\n",
      "4_5_shape2.ipynb\t\t\t\t   launch_feature_extraction.py\r\n",
      "4_shape_analysis.ipynb\t\t\t\t   modify_patch_db.ipynb\r\n",
      "5_3d_align.ipynb\t\t\t\t   nuclei_pipeline\r\n",
      "6_ink_filter_practice.ipynb\t\t\t   tissue_quality\r\n",
      "7_align_slides.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../ArcticAI_Prototype/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate Mapper graphs macro+tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ink prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blend predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize / cut sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate alignment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply slide level alignment parameters to each exported image + mask\n",
    "# original, macro, tumor, nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more here; turn each cell into python script after testing\n",
    "# arcticai package\n",
    "# airflow scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "def filter_mask(mask): # fill holes here (2 masks output) and ensure only have top X sections; find largest sections; do later\n",
    "    macro_mask=fill_holes(mask)\n",
    "    return mask, macro_mask\n",
    "\n",
    "# holes for certain analysis type; rip from pathflow and lower dependencies\n",
    "# preprocess\n",
    "# analysis_type=\"\"\n",
    "# turn into custom dataset for pathpretrain eval\n",
    "# predict(hidden_topology=[32]*3)\n",
    "# predict_nuclei()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
