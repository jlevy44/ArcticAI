{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npy\n",
    "# turn to npy to do all of these in parallel, one section at a time, white space non tissue\n",
    "# ink removal\n",
    "# label expansion with bayes\n",
    "# downsize original and labeled image? no! maybe by 2\n",
    "\n",
    "# notes\n",
    "# ink removal here, in future at beginning\n",
    "# next script, stitch and open sea dragon; v2 go 3D model; start with patient and do ordering; rotation matrix from shapes\n",
    "# stain norm at beginning of entire pipeline\n",
    "# stain norm here? in future do at beginning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given posterior distribution ; may also help out with histobayes\n",
    "# randomly sample from predictive posterior during training\n",
    "# add training \n",
    "# new graph construction given more datapoints in between\n",
    "# single graph prediction propagation, bce with probability\n",
    "# trim new points at beginning with tissue mask to avoid grabbing 256 over, which propagates to rest of slide, else maybe initialize with argmaxed label propagation first\n",
    "\n",
    "# start with one graph\n",
    "\n",
    "\n",
    "import copy, torch, os\n",
    "from sklearn.decomposition import PCA\n",
    "import copy, numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import product\n",
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import APPNP, AGNNConv, GATConv\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import matplotlib,matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi']=300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcc/predictions/4.predictions.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_layers=3,K=5,alpha=0.2,self_loops=False):\n",
    "        super().__init__()\n",
    "        assert n_layers>=2\n",
    "        self.propagate=nn.ModuleList([APPNP(K=K,alpha=alpha,add_self_loops=self_loops) for i in range(n_layers)])#AGNNConv()\n",
    "        \n",
    "    def forward(self,x,edge_index,mask):\n",
    "        x_mask=x[mask]\n",
    "        for layer in self.propagate[:-1]:\n",
    "            x=layer(x,edge_index)\n",
    "            x_mask=x[mask]\n",
    "            # relu?\n",
    "        x_mask=x[mask]\n",
    "        return self.propagate[-1](x,edge_index)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no_pretrain=False\n",
    "j=4\n",
    "cv_splits='bcc/graph_datasets/cv_splits_256_pretrain.pkl'\n",
    "cv_splits=pickle.load(open(cv_splits,'rb'))\n",
    "\n",
    "prediction_file=\"bcc/predictions/4.predictions.pth\"\n",
    "graph_dataset=pickle.load(open(\"bcc/graph_datasets/pretrain_graph_data_256.pkl\",'rb'))\n",
    "print(prediction_file)\n",
    "val_idx=cv_splits[j]['train_idx']\n",
    "test_idx=cv_splits[j]['val_idx']\n",
    "graphs=torch.load(prediction_file)\n",
    "\n",
    "\n",
    "val_graphs=[graphs[i] for i in val_idx]#[:len(val_idx)]\n",
    "test_graphs=[graphs[i] for i in test_idx]#graphs#[len(val_idx):]\n",
    "all_graphs=val_graphs+test_graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components=(graph_dataset['df']['ID']+\"_\"+graph_dataset['df']['component'].astype(str)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.Series(np.vectorize(lambda x: x.split('_')[0])(components)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=\"163\"\n",
    "# df_100=graph_dataset['df'].loc[np.vectorize(lambda x: x.split('_')[0]==k)(graph_dataset['df']['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"163_A1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_100.groupby(['ID','component']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np, cv2\n",
    "# arr=cv2.resize(np.load(\"bcc/inputs/163_B2b.npy\"),None,fx=1/6,fy=1/6)#cv2.cvtColor(,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx\n",
    "#np.hstack([val_idx,test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [slide.id for slide in graph_dataset['graph_dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "# [(x,y) for x,y in ]\n",
    "\n",
    "# val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ps aux | grep f003k8w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_dataset['graph_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# graph_ids\n",
    "# I maybe need to trace back each component ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2,\n",
       "       0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 0, 1, 0, 1, 2, 3, 4, 0, 1, 0, 1,\n",
       "       2, 3, 4, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 0,\n",
       "       1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0, 1, 2, 3, 4, 5,\n",
       "       0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1,\n",
       "       2, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 0, 1, 2, 3, 0,\n",
       "       1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2,\n",
       "       0, 1, 2, 0, 1, 0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 0,\n",
       "       1, 2, 0, 1, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3,\n",
       "       4, 5, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 0, 1, 0, 1, 2,\n",
       "       3, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4,\n",
       "       5, 0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 0, 1, 2, 3,\n",
       "       0, 1, 2, 3, 0, 1, 0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 0, 1, 2, 0,\n",
       "       1, 2, 3, 4, 5, 0, 1, 2, 0, 1, 2, 3, 0, 1, 0, 1, 2, 3, 4, 5, 0, 1,\n",
       "       2, 3, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 3, 0,\n",
       "       1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 0,\n",
       "       1, 2, 3, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_graphs\n",
    "# basename=\"163_A1a\"\n",
    "# graph_ids==basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 81])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.where(graph_ids==basename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Slide\n",
      "Processing Slide [1/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 867357/867357 [02:34<00:00, 5623.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Slide [2/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 845684/845684 [02:29<00:00, 5669.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending Final Image\n",
      "Loading Slide\n",
      "Processing Slide [1/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880726/880726 [02:42<00:00, 5429.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Slide [2/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 864673/864673 [02:23<00:00, 6005.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending Final Image\n",
      "Loading Slide\n",
      "Processing Slide [1/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840019/840019 [02:29<00:00, 5614.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Slide [2/2]\n",
      "Getting new interpolation points\n",
      "Propagating Probability Map\n",
      "Applying Labels to Map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 831614/831614 [02:27<00:00, 5636.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending Final Image\n"
     ]
    }
   ],
   "source": [
    "# for now just consider marginalized posterior, in future, add random sampling from posterior\n",
    "# loss for now will be MSE to logits???\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "# too slow, need to execute this all in parallel in separate nodes and combine results\n",
    "\n",
    "graph_ids=np.array(list(map(lambda x: x.id,graph_dataset['graph_dataset'])))#[val_idx]\n",
    "components=np.zeros(len(graph_ids))#[np.arange(n_times)]\n",
    "# n_times=dict(pd.Series(graph_ids).value_counts())\n",
    "for ID in np.unique(graph_ids):\n",
    "    slide_bool=graph_ids==ID\n",
    "    n_times=sum(slide_bool)\n",
    "    components[slide_bool]=np.arange(n_times)\n",
    "components=components.astype(int)\n",
    "components=components[np.hstack((val_idx,test_idx))]\n",
    "graph_ids=graph_ids[np.hstack((val_idx,test_idx))]\n",
    "\n",
    "threshold=225\n",
    "n_components=2\n",
    "base_nm=\"163_A1*\"\n",
    "files=sorted(glob.glob(f\"bcc/inputs/{base_nm}.npy\"),key=lambda x: x[-5])\n",
    "for f in files:\n",
    "    basename=os.path.basename(f).replace(\".npy\",\"\")#\"140_A1c\"\n",
    "    print(\"Loading Slide\")\n",
    "    arr=np.load(f\"bcc/inputs/{basename}.npy\")\n",
    "    arr[cv2.cvtColor(arr,cv2.COLOR_RGB2GRAY)>threshold,:]=255\n",
    "    pred_map=np.zeros(arr.shape[:2])\n",
    "    division_map=np.zeros(arr.shape[:2])\n",
    "    \n",
    "    slide_idx=np.where(graph_ids==basename)[0]\n",
    "    n_components=len(slide_idx)\n",
    "    \n",
    "    for k in slide_idx:# components[graph_ids==basename]:#range(n_components):\n",
    "        j=components[k]\n",
    "        print(f\"Processing Slide [{j+1}/{n_components}]\")\n",
    "    #     j=0\n",
    "#         idx=val_idx[j]\n",
    "\n",
    "        graph=all_graphs[k]\n",
    "\n",
    "        print(\"Getting new interpolation points\")\n",
    "\n",
    "        xmin,ymin=graph['xy'].min(0).astype(int)#graph_dataset['graph_dataset'][idx].pos.values.numpy()\n",
    "        xmax,ymax=graph['xy'].max(0).astype(int)\n",
    "\n",
    "        grid=np.array(list(product(range(xmin,xmax,32),range(ymin,ymax,32))))\n",
    "\n",
    "        G=radius_neighbors_graph(np.vstack([graph['xy'],grid]),radius=16) # then deduplicate and add .. hmm maybe just build off of current graph\n",
    "        grid=np.delete(grid,G.nonzero()[1][:int(len(G.nonzero()[0])/2)]-len(graph['xy']),axis=0)\n",
    "\n",
    "        G=radius_neighbors_graph(np.vstack([graph['xy'],grid]),radius=256*np.sqrt(2)) # then deduplicate and add .. hmm maybe just build off of current graph\n",
    "\n",
    "        new_pos=np.vstack([graph['xy'],grid])[np.where(np.array(G[:,:len(graph['xy'])].sum(1)>0).flatten())[0],:]\n",
    "\n",
    "        G=radius_neighbors_graph(new_pos,radius=32*np.sqrt(2))\n",
    "\n",
    "        edge_index=from_scipy_sparse_matrix(G)[0]\n",
    "\n",
    "        x_old=graph['y_pred']\n",
    "        knn=KNeighborsClassifier(n_neighbors=1).fit(graph['xy'],x_old.argmax(1))\n",
    "        prior=knn.predict(new_pos[len(graph['xy']):])\n",
    "        x_new=np.zeros((G.shape[0]-len(graph['xy']),x_old.shape[1]))\n",
    "        x_new[:,1]=prior\n",
    "        x_new[:,0]=1-prior\n",
    "        # \n",
    "        x=torch.tensor(np.vstack([x_old,x_new])).float()\n",
    "        mask=np.array([False]*len(x))\n",
    "        mask[:len(graph['xy'])]=True\n",
    "\n",
    "        print(\"Propagating Probability Map\")\n",
    "\n",
    "        graph_data=Data(x=x,edge_index=edge_index,mask=torch.tensor(mask),pos=torch.tensor(new_pos))\n",
    "\n",
    "        graph_data.x=graph_data.x.cuda()\n",
    "        graph_data.mask=graph_data.mask.cuda()\n",
    "        graph_data.edge_index=graph_data.edge_index.cuda()\n",
    "\n",
    "        net=Net(n_layers=30,\n",
    "               K=15,\n",
    "               alpha=0.1,\n",
    "               self_loops=False).cuda()\n",
    "\n",
    "        y_pred=F.softmax(net(graph_data.x,graph_data.edge_index,graph_data.mask))\n",
    "\n",
    "#         idx=val_idx[j]\n",
    "        # graph_dataset['graph_dataset'][idx].id\n",
    "\n",
    "        preds=y_pred.detach().cpu().numpy()[:,1]\n",
    "\n",
    "        print(\"Applying Labels to Map\")\n",
    "\n",
    "        patch_size=256\n",
    "        for i,(x,y) in tqdm.tqdm(list(enumerate(graph_data.pos.numpy().astype(int).tolist()))):\n",
    "            pred_map[x:x+patch_size,y:y+patch_size]+=preds[i]\n",
    "            division_map[x:x+patch_size,y:y+patch_size]+=1.\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Blending Final Image\")\n",
    "    division_map[division_map==0]=1\n",
    "    pred_map=pred_map/division_map\n",
    "    del division_map\n",
    "    # arr=cv2.resize(arr,None,fx=1/6.,fy=1/6.)\n",
    "    small_pred_map=pred_map#cv2.resize(pred_map,None,fx=1/6.,fy=1/6.)\n",
    "    new_pred_map = cv2.applyColorMap((small_pred_map*255.).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    # ADD GAUSSIAN BLUR HERE ^^^ in future\n",
    "    alpha=0.3\n",
    "    blended_image=(arr*(1-alpha)+(255-new_pred_map)*alpha).astype(np.uint8)\n",
    "    blended_image[small_pred_map==0,:]=arr[small_pred_map==0,:]\n",
    "\n",
    "    for j,k in enumerate(np.argsort([all_graphs[slide_idx[j]]['xy'].max(0)[1] for j in range(n_components)]).tolist()):#val_graphs[j]\n",
    "        graph=all_graphs[slide_idx[k]]#val_graphs[k]\n",
    "        xmin,ymin=graph['xy'].min(0).astype(int)#graph_dataset['graph_dataset'][idx].pos.values.numpy()\n",
    "        xmax,ymax=graph['xy'].max(0).astype(int)\n",
    "        cv2.imwrite(f\"bcc/interactive_test/{basename}.img.{j}.png\",cv2.cvtColor(arr[xmin:xmax,ymin:ymax],cv2.COLOR_BGR2RGB))\n",
    "        cv2.imwrite(f\"bcc/interactive_test/{basename}.prob_map.{j}.png\",cv2.cvtColor(blended_image[xmin:xmax,ymin:ymax],cv2.COLOR_BGR2RGB))\n",
    "    del blended_image, pred_map, new_pred_map, small_pred_map\n",
    "# next is alignment program & remove white space & combine with other slides alphabetically\n",
    "# ^ align & ink & white space & order\n",
    "# then openseadragon, but try this one next\n",
    "\n",
    "# for j in range(3):\n",
    "\n",
    "# cut up here, add ordering, which will integrate in final stitching with dynamic alignment\n",
    "\n",
    "# plt.imshow(blended_image)\n",
    "\n",
    "# fix right here initialize label with original neighbor\n",
    "\n",
    "# WHY DO PROBABILITY MAPS CHANGE EVERY ITERATION!!!! HELP!!!! :O FIX FIX FIX See slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls bcc/interactive_test\n",
    "# import subprocess,glob\n",
    "\n",
    "# for f in glob.glob(\"bcc/interactive_test/*.png\"):\n",
    "#     subprocess.call(\"mv {} {}\".format(f,os.path.join(os.path.dirname(f),f\"{basename}.{os.path.basename(f)}\")),shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir bcc/interactive_test\n",
    "# ! pip install git+https://github.com/openzoom/deepzoom.py.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lah bcc/interactive_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_graphs[1]['xy'].max(0)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_graphs[2]['xy'].max(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.resize(blended_image,None,fx=1/6.,fy=1/6.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide ordering -> pngx2 per section\n",
    "# then for each section, tissue mask and point cloud alignment or airlab, borrow tissue mask to crop the probability counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha=0.3\n",
    "# blended_image=(arr*(1-alpha)+(255-new_pred_map)*alpha).astype(np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blended_image[small_pred_map==0,:]=arr[small_pred_map==0,:]\n",
    "# plt.imshow((arr*(1-alpha)+(255-new_pred_map)*alpha).astype(np.uint8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(blended_image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/snorkel-team/snorkel/master/snorkel/classification/loss.py\n",
    "\n",
    "from typing import List, Mapping, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Outputs = Mapping[str, List[torch.Tensor]]\n",
    "\n",
    "\n",
    "def cross_entropy_with_probs(\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    weight: Optional[torch.Tensor] = None,\n",
    "    reduction: str = \"mean\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Calculate cross-entropy loss when targets are probabilities (floats), not ints.\n",
    "\n",
    "    PyTorch's F.cross_entropy() method requires integer labels; it does accept\n",
    "    probabilistic labels. We can, however, simulate such functionality with a for loop,\n",
    "    calculating the loss contributed by each class and accumulating the results.\n",
    "    Libraries such as keras do not require this workaround, as methods like\n",
    "    \"categorical_crossentropy\" accept float labels natively.\n",
    "\n",
    "    Note that the method signature is intentionally very similar to F.cross_entropy()\n",
    "    so that it can be used as a drop-in replacement when target labels are changed from\n",
    "    from a 1D tensor of ints to a 2D tensor of probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input\n",
    "        A [num_points, num_classes] tensor of logits\n",
    "    target\n",
    "        A [num_points, num_classes] tensor of probabilistic target labels\n",
    "    weight\n",
    "        An optional [num_classes] array of weights to multiply the loss by per class\n",
    "    reduction\n",
    "        One of \"none\", \"mean\", \"sum\", indicating whether to return one loss per data\n",
    "        point, the mean loss, or the sum of losses\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The calculated loss\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If an invalid reduction keyword is submitted\n",
    "    \"\"\"\n",
    "    num_points, num_classes = input.shape\n",
    "    # Note that t.new_zeros, t.new_full put tensor on same device as t\n",
    "    cum_losses = input.new_zeros(num_points)\n",
    "    for y in range(num_classes):\n",
    "        target_temp = input.new_full((num_points,), y, dtype=torch.long)\n",
    "        y_loss = F.cross_entropy(input, target_temp, reduction=\"none\")\n",
    "        if weight is not None:\n",
    "            y_loss = y_loss * weight[y]\n",
    "        cum_losses += target[:, y].float() * y_loss\n",
    "\n",
    "    if reduction == \"none\":\n",
    "        return cum_losses\n",
    "    elif reduction == \"mean\":\n",
    "        return cum_losses.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return cum_losses.sum()\n",
    "    else:\n",
    "        raise ValueError(\"Keyword 'reduction' must be one of ['none', 'mean', 'sum']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#y_pred=F.softmax(net(graph_data.x,graph_data.edge_index)).detach().numpy() # ???)\n",
    "# plt.scatter(*graph_data.pos.numpy().T.tolist(),c=y_pred.argmax(1),s=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good enough for now, fix the outside points\n",
    "\n",
    "\n",
    "plt.scatter(*graph_data.pos.numpy().T.tolist(),c=y_pred.detach().cpu().numpy()[:,1],s=1,alpha=0.2)#.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*graph['xy'].T.tolist(),c=graph['y_pred'][:,1],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*graph['xy'].T.tolist(),c=graph['y_pred'].argmax(axis=1),s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*graph['xy'].T.tolist(),c=graph['y'],s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of them from the same slide, then do slicing, then produce mixed image and unmixed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(new_pred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.3\n",
    "plt.imshow((arr*(1-alpha)+(255-new_pred_map)*alpha).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_map.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as opt\n",
    "\n",
    "# optimizer=opt.Adam(net.parameters(),lr=1e-2)\n",
    "# for i in range(5000):\n",
    "#     y_true=F.softmax(graph_data.x[graph_data.mask])\n",
    "#     y_pred=net(graph_data.x,graph_data.edge_index,graph_data.mask)[graph_data.mask]\n",
    "#     loss=cross_entropy_with_probs(y_pred,y_true)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(loss.item())\n",
    "#     del y_pred,loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[len(graph['xy']):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.detach().cpu().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a naive untrained GNN adding layers for label propagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,j in G.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset['graph_dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
