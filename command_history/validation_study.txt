/dartfs/rc/lab/V/VaickusL_slow/users/jlevy/arctic_ai/prototype_validation_study
# 0. pull cases, set up directories
0_case_list_dir_setup.ipynb

# 1. extract patch information
for basename in $(comm -3 <( ls inputs/*.npy | xargs -I F basename F .npy | sort ) <(ls patch_info/tumor/*.pkl | xargs -I F basename F .pkl | sort)); do submit-job run_torque_job -c "singularity  exec --nv -B /scratch/ -B $(realpath ../../..)  --bind ${HOME}:/mnt /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img python 0_extract_patch_info.py --basename ${basename}" -i "conda activate pathflow" -ao "-A QDP-Alpha -l nodes=1:ppn=8 -l feature=v100" -t 1 -n 0 -q gpuq ; done
# update
for basename in $(comm -3 <( ls new_test_slides/ASAP_Tiff/*.tif | xargs -I F basename F _ASAP.tif | sort ) <(ls updated_patch_info/tumor/*.pkl | xargs -I F basename F .pkl | sort)); do submit-job run_torque_job -c "singularity  exec --nv -B /scratch/ -B $(realpath ../../..)  --bind ${HOME}:/mnt /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img python 0_extract_patch_info.py --basename ${basename} --input_dir new_test_slides/ASAP_Tiff/ --output_dir updated_patch_info/" -i "conda activate pathflow" -ao "-A QDP-Alpha -l nodes=1:ppn=1 -l feature=v100" -t 1 -n 0 -q gpuq ; done

for basename in $(comm -3 <( ls new_skin_layers/Skin_Layer_ASAP_TIFF/*.tif | xargs -I F basename F _ASAP.tif | sort ) <(ls updated_patch_info_skin_layers/tumor/*.pkl | xargs -I F basename F .pkl | sort)); do submit-job run_torque_job -c "singularity  exec --nv -B /scratch/ -B $(realpath ../../..)  --bind ${HOME}:/mnt /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img python 0_extract_patch_info.py --basename ${basename} --input_dir new_skin_layers/Skin_Layer_ASAP_TIFF/ --output_dir updated_patch_info_skin_layers/" -i "conda activate pathflow" -ao "-A QDP-Alpha -l nodes=1:ppn=1 -l feature=v100" -t 1 -n 0 -q gpuq ; done


# 2. apply annotations to patch information; verify annotations okay based on overlap with patch info
1_apply_annotations.ipynb

# 3. generate CNN inputs
2_write_CNN_data.ipynb

# 4. partition datasets (separate test set; one train/val/maybe test with patient clustering; split by amount of tumor dichotomized; separate test set no annotations by excel)
2_write_CNN_data.ipynb # see splits in pickle datasets

# 5. train CNN models
# first pass
# pretrained
export SINGULARITYENV_PREPEND_PATH=/dartfs-hpc/rc/home/w/f003k8w/.local/bin/
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir patches/macro/ --checkpoints_dir checkpoints_cnn_macro/  --num_classes 4 --verbose True --batch_size 256 --model_save_loc macro_cnn.pkl --gpu_id 2 --pickle_dataset True &
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir patches/tumor/ --checkpoints_dir checkpoints_cnn_tumor/  --num_classes 3 --verbose True --batch_size 256 --model_save_loc tumor_cnn.pkl --gpu_id 3 --pickle_dataset True &
# finetune
export SINGULARITYENV_PREPEND_PATH=/dartfs-hpc/rc/home/w/f003k8w/.local/bin/
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir patches/macro/ --checkpoints_dir checkpoints_cnn_macro_round_2/ --pretrained_save_loc checkpoints_cnn_macro/16.epoch.checkpoint.pth --num_classes 4 --verbose True --batch_size 256 --model_save_loc macro_cnn.pkl --gpu_id 2 --pickle_dataset True &
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir patches/tumor/ --checkpoints_dir checkpoints_cnn_tumor_round_2/ --pretrained_save_loc checkpoints_cnn_tumor/9.epoch.checkpoint.pth  --num_classes 3 --verbose True --batch_size 256 --model_save_loc tumor_cnn.pkl --gpu_id 3 --pickle_dataset True &
# predict on train, val
singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_train_tumor_tmp.pkl --class_balance True --inputs_dir patches/tumor/ --checkpoints_dir checkpoints_cnn_tumor_round_2/ --predict True --predict_set train --model_save_loc checkpoints_cnn_tumor_round_2/9.epoch.checkpoint.pth  --num_classes 3 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True
singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_train_macro_tmp.pkl --class_balance True --inputs_dir patches/macro/ --checkpoints_dir checkpoints_cnn_macro_round_2/ --predict True --predict_set train --model_save_loc checkpoints_cnn_macro_round_2/0.epoch.checkpoint.pth  --num_classes 4 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True
singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_val_tumor_tmp.pkl --class_balance True --inputs_dir patches/tumor/ --checkpoints_dir checkpoints_cnn_tumor_round_2/ --predict True --predict_set val --model_save_loc checkpoints_cnn_tumor_round_2/9.epoch.checkpoint.pth  --num_classes 3 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True
singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_val_macro_tmp.pkl --class_balance True --inputs_dir patches/macro/ --checkpoints_dir checkpoints_cnn_macro_round_2/ --predict True --predict_set val --model_save_loc checkpoints_cnn_macro_round_2/0.epoch.checkpoint.pth  --num_classes 4 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True

# second pass
export SINGULARITYENV_PREPEND_PATH=/dartfs-hpc/rc/home/w/f003k8w/.local/bin/
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir updated_patches/macro/ --checkpoints_dir updated_checkpoints_cnn_macro_v2/  --num_classes 4 --verbose True --batch_size 256 --model_save_loc updated_macro_cnn.pkl --gpu_id 2 --pickle_dataset True &
nohup singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --class_balance True --inputs_dir updated_patches/tumor/ --checkpoints_dir updated_checkpoints_cnn_tumor/  --num_classes 3 --verbose True --batch_size 256 --model_save_loc updated_tumor_cnn.pkl --gpu_id 3 --pickle_dataset True &

singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_val_tumor_tmp.pkl --class_balance True --inputs_dir updated_patches/tumor/ --checkpoints_dir updated_checkpoints_cnn_tumor/ --predict True --predict_set val --model_save_loc updated_checkpoints_cnn_tumor/30.epoch.checkpoint.pth  --num_classes 3 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True
singularity exec --nv  -B /scratch/ -B $(realpath ../../../..)  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --predictions_save_path pred_val_macro_tmp.pkl --class_balance True --inputs_dir updated_patches/macro/ --checkpoints_dir updated_checkpoints_cnn_macro_v2/ --predict True --predict_set val --model_save_loc updated_checkpoints_cnn_macro_v2/24.epoch.checkpoint.pth  --num_classes 4 --verbose True --batch_size 256 --gpu_id 3 --pickle_dataset True

# 6. extract embeddings

new_test_slides/ASAP_Tiff/
new_skin_layers/Skin_Layer_ASAP_TIFF/
sed -e 's/.*-gpu//g'
#&& unset CUDA_VISIBLE_DEVICES && export CUDA_VISIBLE_DEVICES=\${gpuNum}


for f in $(ls new_test_slides/new_tiff/*.tif | xargs -I F basename F .tif | sort ); do  submit-job run_torque_job -c "export SINGULARITYENV_CUDA_VISIBLE_DEVICES=\${gpuNum} && export SINGULARITYENV_PREPEND_PATH=/dartfs-hpc/rc/home/w/f003k8w/.local/bin/ && singularity exec --nv -B /dartfs/rc/lab/V/VaickusL_slow/  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --tensor_dataset False --pickle_dataset True --inputs_dir updated_patches/tumor/ --checkpoints_dir updated_checkpoints_cnn_tumor --num_classes 3 --verbose True --batch_size 32 --model_save_loc updated_checkpoints_cnn_tumor/30.epoch.checkpoint.pth --embedding_out_dir cnn_embeddings/tumor/ --extract_embeddings_df updated_model_patch_annotations/tumor_annot_gnn_annot.pkl --extract_embeddings new_test_slides/new_tiff/${f}.tif --predict True" -a "gpuNum=\$(cat \$PBS_GPUFILE | tail -c 2)" -a "echo \${gpuNum} " -ao "-A QDP-Alpha -l nodes=1:ppn=8:gpus=1 -l feature=v100" -t 1 -n 0 -q "gpuq" ; done

for f in $(ls new_skin_layers/skin_layer_tiff/*.tif | xargs -I F basename F .tif | sort ); do  submit-job run_torque_job -c "export SINGULARITYENV_CUDA_VISIBLE_DEVICES=\${gpuNum} && export SINGULARITYENV_PREPEND_PATH=/dartfs-hpc/rc/home/w/f003k8w/.local/bin/ && singularity exec --nv -B /dartfs/rc/lab/V/VaickusL_slow/  --bind ${HOME}:/mnt  /dartfs/rc/lab/V/VaickusL_slow/singularity_containers/PathFlow/pathflowgcn_new.img pathpretrain --tensor_dataset False --pickle_dataset True --inputs_dir updated_patches/macro/ --checkpoints_dir updated_checkpoints_cnn_macro_v2 --num_classes 4 --verbose True --batch_size 32 --model_save_loc updated_checkpoints_cnn_macro_v2/24.epoch.checkpoint.pth --embedding_out_dir cnn_embeddings/macro/ --extract_embeddings_df updated_model_patch_annotations/macro_annot_gnn_annot.pkl --extract_embeddings new_skin_layers/skin_layer_tiff/${f}.tif --predict True" -a "gpuNum=\$(cat \$PBS_GPUFILE | tail -c 2)" -a "echo \${gpuNum} " -ao "-A QDP-Alpha -l nodes=1:ppn=8:gpus=1 -l feature=v100" -t 1 -n 0 -q "gpuq" ; done

# 7. generate GNN inputs, thumbnails
# create db file first, then patch embedding extraction

# 8. train GNN models

# 9. Future: train nuclei model, don't do for now

# 10. test on held-out test set

# set up airflow -> airflow via conda first, then pip airflow, then mysqlclient, then submit_hpc
pip install apache-airflow --upgrade
conda activate airflow
export AIRFLOW_HOME=/dartfs/rc/lab/V/VaickusL_slow/users/jlevy/arctic_ai/prototype/ArcticAI_Prototype/workflow/airflow
https://rc.dartmouth.edu/index.php/hrf_faq/accessing-your-research-mysql-database/
https://www.hpc.iastate.edu/guides/containers/mysql-server
singularity pull --name mysql.simg shub://ISU-HPC/mysql
singularity instance start --bind ${HOME}     --bind ${PWD}/mysql/var/lib/mysql/:/var/lib/mysql     --bind ${PWD}/mysql/run/mysqld:/run/mysqld     ./mysql.simg mysql
nano /dartfs-hpc/rc/home/w/f003k8w/.my.cnf
# ^^^generate tmp dir https://stackoverflow.com/questions/8633373/mysql-cant-create-write-to-file-error13
# ^^^generate explicit_defaults_for_timestamp = 1
singularity run instance://mysql
singularity exec instance://mysql create_remote_admin_user.sh
username: remote_usr
password: Lk9r9U0-
#sql_alchemy_conn = mysql://root:airflow@localhost/airflow?unix_socket=/dartfs/rc/lab/V/VaickusL_slow/users/jlevy/arctic_ai/prototype/ArcticAI_Prototype/workflow/mysql/run/mysqld/mysqld.sock
nano airflow/airflow.cfg
sql_alchemy_conn = mysql+mysqldb://remote_usr:Lk9r9U0-@localhost/airflow?unix_socket=mysql/run/mysqld/mysqld.sock
nano airflow/airflow.cfg
executor = LocalExecutor
openssl rand -hex 30 # lookup if stuck
secret_key = 78455bc775c77ee544de4cd6b862630cbc4ed890491daabedf34cf78e68c
cookie_samesite = Lax
# https://raw.githubusercontent.com/apache/airflow/51d955787b009b9e3a88f3e9b4ca1a3933a061f0/airflow/config_templates/default_webserver_config.py
nano airflow/webserver_config.py
CSRF_ENABLED = True


mysql -S mysql/run/mysqld/mysqld.sock
CREATE DATABASE airflow CHARACTER SET utf8 COLLATE utf8_unicode_ci;
CREATE USER 'airflow' IDENTIFIED BY 'airflow';
GRANT ALL PRIVILEGES ON airflow.* TO 'airflow';
# singularity instance stop mysql

airflow db init
# airflow users delete --username admin
airflow users create \
    --username admin \
    --firstname Joshua \
    --lastname Levy \
    --role Admin \
    --email joshualevy44@yahoo.com
Password: password

airflow scheduler > scheduler.log &
airflow webserver --port 5559


###########
OLD
https://hub.docker.com/_/mysql
singularity pull docker://mysql
singularity shell --writable-tmpfs -B /dartfs/rc/lab/V/VaickusL_slow/ -B $(pwd)/mysql_db/:/var/lib/mysql mysql_latest.sif
mysqld_safe --datadir=/var/lib/mysql &
mysql_install_db
mysql_secure_installation
password
