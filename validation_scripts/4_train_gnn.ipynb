{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f93c244] validation update\n",
      " Committer: Joshua J. Levy <f003k8w@p03.hpcc.dartmouth.edu>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly:\n",
      "\n",
      "    git config --global user.name \"Your Name\"\n",
      "    git config --global user.email you@example.com\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 2 files changed, 108 insertions(+), 14 deletions(-)\n",
      "Counting objects: 4, done.\n",
      "Delta compression using up to 80 threads.\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 1.37 KiB | 279.00 KiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/jlevy44/ArcticAI_Prototype\n",
      "   4128d6d..f93c244  master -> master\n"
     ]
    }
   ],
   "source": [
    "# ! ln *py* ../prototype/ArcticAI_Prototype/validation_scripts/\n",
    "! cd ../prototype/ArcticAI_Prototype/ && git add * */* && git commit -a -m \"validation update\" && git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "import sys, os\n",
    "sys.path.insert(0,\"/dartfs-hpc/rc/home/w/f003k8w/.local/lib/python3.7/site-packages/\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_cluster import knn_graph\n",
    "from torch_geometric.data import Data \n",
    "import numpy as np\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import os\n",
    "import argparse\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import InMemoryDataset,DataLoader\n",
    "import os,glob, pandas as pd\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, DeepGraphInfomax, SAGEConv\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import APPNP\n",
    "import torch.nn as nn\n",
    "import fire\n",
    "from visdom import Visdom\n",
    "from torch_geometric.nn import GENConv, DeepGCNLayer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "\n",
    "def dense_mincut_pool(x, adj, s, mask=None):\n",
    "    r\"\"\"MinCUt pooling operator from the `\"Mincut Pooling in Graph Neural\n",
    "    Networks\" <https://arxiv.org/abs/1907.00481>`_ paper\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} &= {\\mathrm{softmax}(\\mathbf{S})}^{\\top} \\cdot\n",
    "        \\mathbf{X}\n",
    "        \\mathbf{A}^{\\prime} &= {\\mathrm{softmax}(\\mathbf{S})}^{\\top} \\cdot\n",
    "        \\mathbf{A} \\cdot \\mathrm{softmax}(\\mathbf{S})\n",
    "    based on dense learned assignments :math:`\\mathbf{S} \\in \\mathbb{R}^{B\n",
    "    \\times N \\times C}`.\n",
    "    Returns pooled node feature matrix, coarsened symmetrically normalized\n",
    "    adjacency matrix and two auxiliary objectives: (1) The minCUT loss\n",
    "    .. math::\n",
    "        \\mathcal{L}_c = - \\frac{\\mathrm{Tr}(\\mathbf{S}^{\\top} \\mathbf{A}\n",
    "        \\mathbf{S})} {\\mathrm{Tr}(\\mathbf{S}^{\\top} \\mathbf{D}\n",
    "        \\mathbf{S})}\n",
    "    where :math:`\\mathbf{D}` is the degree matrix, and (2) the orthogonality\n",
    "    loss\n",
    "    .. math::\n",
    "        \\mathcal{L}_o = {\\left\\| \\frac{\\mathbf{S}^{\\top} \\mathbf{S}}\n",
    "        {{\\|\\mathbf{S}^{\\top} \\mathbf{S}\\|}_F} -\\frac{\\mathbf{I}_C}{\\sqrt{C}}\n",
    "        \\right\\|}_F.\n",
    "    Args:\n",
    "        x (Tensor): Node feature tensor :math:`\\mathbf{X} \\in \\mathbb{R}^{B\n",
    "            \\times N \\times F}` with batch-size :math:`B`, (maximum)\n",
    "            number of nodes :math:`N` for each graph, and feature dimension\n",
    "            :math:`F`.\n",
    "        adj (Tensor): Symmetrically normalized adjacency tensor\n",
    "            :math:`\\mathbf{A} \\in \\mathbb{R}^{B \\times N \\times N}`.\n",
    "        s (Tensor): Assignment tensor :math:`\\mathbf{S} \\in \\mathbb{R}^{B\n",
    "            \\times N \\times C}` with number of clusters :math:`C`. The softmax\n",
    "            does not have to be applied beforehand, since it is executed\n",
    "            within this method.\n",
    "        mask (BoolTensor, optional): Mask matrix\n",
    "            :math:`\\mathbf{M} \\in {\\{ 0, 1 \\}}^{B \\times N}` indicating\n",
    "            the valid nodes for each graph. (default: :obj:`None`)\n",
    "    :rtype: (:class:`Tensor`, :class:`Tensor`, :class:`Tensor`,\n",
    "        :class:`Tensor`)\n",
    "    \"\"\"\n",
    "\n",
    "    x = x.unsqueeze(0) if x.dim() == 2 else x\n",
    "    adj = adj.unsqueeze(0) if adj.dim() == 2 else adj\n",
    "    s = s.unsqueeze(0) if s.dim() == 2 else s\n",
    "\n",
    "    (batch_size, num_nodes, _), k = x.size(), s.size(-1)\n",
    "\n",
    "    s = torch.softmax(s, dim=-1)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.view(batch_size, num_nodes, 1).to(x.dtype)\n",
    "        x, s = x * mask, s * mask\n",
    "\n",
    "    out = torch.matmul(s.transpose(1, 2), x)\n",
    "    out_adj = torch.matmul(torch.matmul(s.transpose(1, 2), adj), s)\n",
    "\n",
    "    # MinCUT regularization.\n",
    "    mincut_num = _rank3_trace(out_adj)\n",
    "    d_flat = torch.einsum('ijk->ij', adj)\n",
    "    d = _rank3_diag(d_flat)\n",
    "    mincut_den = _rank3_trace(\n",
    "        torch.matmul(torch.matmul(s.transpose(1, 2), d), s))\n",
    "    mincut_loss = -(mincut_num / mincut_den)\n",
    "    mincut_loss = torch.mean(mincut_loss)\n",
    "\n",
    "    # Orthogonality regularization.\n",
    "    ss = torch.matmul(s.transpose(1, 2), s)\n",
    "    i_s = torch.eye(k).type_as(ss)\n",
    "    ortho_loss = torch.norm(\n",
    "        ss / torch.norm(ss, dim=(-1, -2), keepdim=True,p=2) -\n",
    "        i_s / torch.norm(i_s), dim=(-1, -2),p=2)\n",
    "    ortho_loss = torch.mean(ortho_loss)\n",
    "\n",
    "    # Fix and normalize coarsened adjacency matrix.\n",
    "    ind = torch.arange(k, device=out_adj.device)\n",
    "    out_adj[:, ind, ind] = 0\n",
    "    d = torch.einsum('ijk->ij', out_adj)\n",
    "    d = torch.sqrt(d)[:, None] + EPS\n",
    "    out_adj = (out_adj / d) / d.transpose(1, 2)\n",
    "\n",
    "    return out, out_adj, mincut_loss, ortho_loss\n",
    "\n",
    "\n",
    "def _rank3_trace(x):\n",
    "    return torch.einsum('ijj->i', x)\n",
    "\n",
    "\n",
    "def _rank3_diag(x):\n",
    "    eye = torch.eye(x.size(1)).type_as(x)\n",
    "    out = eye * x.unsqueeze(2).expand(*x.size(), x.size(1))\n",
    "    return out\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        z=x\n",
    "        if self.training:\n",
    "            z = self.dropout(z)\n",
    "        x = self.fc(z)\n",
    "        return x,z\n",
    "    \n",
    "class DeeperGCN(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_output, hidden_channels, num_layers=28, edge_drop_p=0.05, p_dropout=0.1):\n",
    "        super(DeeperGCN, self).__init__()\n",
    "\n",
    "        self.node_encoder = nn.Linear(n_input, hidden_channels)\n",
    "        self.p_dropout=p_dropout\n",
    "#         self.edge_encoder = Linear(data.edge_attr.size(-1), hidden_channels)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=edge_drop_p)[0]\n",
    "        for i in range(1, num_layers + 1):\n",
    "            conv = GENConv(hidden_channels, hidden_channels, aggr='softmax',\n",
    "                           t=1.0, learn_t=True, num_layers=2, norm='layer')\n",
    "            norm = nn.LayerNorm(hidden_channels, elementwise_affine=True)\n",
    "            act = nn.ReLU(inplace=True)\n",
    "\n",
    "            layer = DeepGCNLayer(conv, norm, act, block='res+', dropout=0.1,\n",
    "                                 ckpt_grad=i % 3)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "\n",
    "        self.lin = nn.Linear(hidden_channels, n_output)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.node_encoder(x)\n",
    "#         edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        x = self.layers[0].conv(x, edge_index, None)\n",
    "        orig_edges=edge_index.shape[1]\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x, edge_index)\n",
    "            edge_index=self.drop_edge(edge_index)\n",
    "#         print(edge_index.shape[1]/orig_edges)\n",
    "\n",
    "        z = self.layers[0].act(self.layers[0].norm(x))\n",
    "        x = F.dropout(z, p=self.p_dropout, training=self.training)\n",
    "\n",
    "        return self.lin(x),z\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.gcn.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = F.softmax(self.gcn.fc(x))\n",
    "        return x,y\n",
    "\n",
    "def fit_model(\n",
    "                model_basename=\"gnn_model.pth\",\n",
    "                log_basename=\"gnn_log.pth\",\n",
    "                graph_datasets=None,\n",
    "                thumbnails=None,\n",
    "                use_weights=False,\n",
    "                n_batches_backward=1,\n",
    "                f1_metric='weighted',\n",
    "                n_epochs=1500,\n",
    "                out_dir='gnn_models',\n",
    "                lr=1e-2,\n",
    "                eta_min=1e-4,\n",
    "                T_max=20,\n",
    "                wd=0,\n",
    "                hidden_topology=[32,64,128,128],\n",
    "                p=0.5,\n",
    "                p2=0.3,\n",
    "                burnin=400,\n",
    "                warmup=100,\n",
    "                gpu_id=0,\n",
    "                batch_size=1,\n",
    "                vis_every=0,\n",
    "                port=5555,\n",
    "                num_layers=28,\n",
    "                use_thumbnails=False,\n",
    "                use_deep=False\n",
    "                ):\n",
    "    print(gpu_id); torch.cuda.set_device(gpu_id)\n",
    "    if not vis_every: vis=None\n",
    "    else: vis=Visdom(port=port)\n",
    "    for k in graph_datasets:\n",
    "        for i in range(len(graph_datasets[k])):\n",
    "            graph_datasets[k][i].index=i\n",
    "    train_dataset=graph_datasets['train']\n",
    "    val_dataset=graph_datasets['val']\n",
    "    print(len(train_dataset),len(val_dataset))\n",
    "    y_true=np.hstack(graph.y_true.numpy() for graph in train_dataset)\n",
    "    y_true=y_true[y_true!=-1]\n",
    "    weights=torch.tensor(compute_class_weight('balanced',np.unique(y_true),y_true))\n",
    "\n",
    "\n",
    "    # load model\n",
    "    if not use_deep:\n",
    "        model=GCNNet(train_dataset[0].x.shape[1],\n",
    "                     y_true.max()+1,\n",
    "                     hidden_topology=hidden_topology,p=p,p2=p2)\n",
    "    else:\n",
    "        model=DeeperGCN(train_dataset[0].x.shape[1],\n",
    "                     y_true.max()+1,\n",
    "                     hidden_topology[0],edge_drop_p=p2,p_dropout=p,num_layers=num_layers)\n",
    "    model=model.cuda()\n",
    "\n",
    "    # load optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=eta_min, last_epoch=-1)\n",
    "    criterion=nn.CrossEntropyLoss(weight=torch.tensor(weights).float() if use_weights else None)\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "    # initialize val saving\n",
    "    save_mod=False\n",
    "    past_performance=[0]\n",
    "\n",
    "    # dataloaders\n",
    "    dataloaders={}\n",
    "\n",
    "    dataloaders['train']=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    dataloaders['val']=DataLoader(val_dataset,shuffle=True)\n",
    "    dataloaders['warmup']=DataLoader(train_dataset,shuffle=False)\n",
    "    train_loader=dataloaders['warmup']\n",
    "\n",
    "    n_total_batches=0\n",
    "    train_val_f1=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        Y,Y_Pred=[],[]\n",
    "        for i,data in enumerate(train_loader):\n",
    "            n_total_batches+=1\n",
    "            model.train(True)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            y_out, z=model(x,edge_index)\n",
    "            loss = criterion(y_out[y!=-1], y[y!=-1]) / n_batches_backward\n",
    "            loss.backward()\n",
    "            if n_total_batches%n_batches_backward==0 or (i==len(train_loader.dataset)-1):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            Y_Pred.append(F.softmax(y_out).argmax(1)[y!=-1].detach().cpu().numpy().flatten())\n",
    "            Y.append(y[y!=-1].detach().cpu().numpy().flatten())\n",
    "#             if epoch%vis_every==0 and not i:\n",
    "#                 vis.scatter(data.pos.numpy(),opts=dict(markercolor=(y_pred*255).astype(int),webgl=False,markerborderwidth=0,markersize=5,title=\"Train Predicted {}\".format(data.id[0])),win=\"train_pred\")\n",
    "#                 vis.scatter(data.pos.numpy(),opts=dict(markercolor=y_true*255,webgl=False,markerborderwidth=0,markersize=5,title=\"Train Real {}\".format(data.id[0])),win=\"train_true\")\n",
    "#                 vis.scatter(PCA(n_components=2).fit_transform(z.detach().cpu().numpy()),opts=dict(markercolor=y_true*255,webgl=False,markerborderwidth=0,markersize=5,title=\"Train PCA {}\".format(data.id[0])),win=\"train_pca\")\n",
    "#                 vis.scatter(PCA(n_components=2).fit_transform(x.detach().cpu().numpy()),opts=dict(markercolor=y_true*255,webgl=False,markerborderwidth=0,markersize=5,title=\"Train Original PCA {}\".format(data.id[0])),win=\"train_pca2\")\n",
    "#                 if use_thumbnails:\n",
    "#                     vis.image(thumbnails['train'][data.index].transpose((2,1,0))[:,::-1,:],win=\"train_image\")\n",
    "            del x, edge_index, loss, y_out, z\n",
    "            if epoch <=warmup:\n",
    "                break \n",
    "        if epoch == warmup:\n",
    "            train_loader=dataloaders['train']\n",
    "        if epoch>=burnin:\n",
    "            save_mod=True\n",
    "        train_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        scheduler.step()\n",
    "        Y,Y_Pred,Y_prob=[],[],[]\n",
    "        for i,data in enumerate(dataloaders['val']):\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            y_out,z=model(x,edge_index)\n",
    "#             loss = criterion(y_out, y) \n",
    "            y_prob=F.softmax(y_out).detach().cpu().numpy()\n",
    "            y_pred=y_prob.argmax(1).flatten()\n",
    "            y_true=y.detach().cpu().numpy().flatten()\n",
    "            Y_Pred.append(y_pred[y_true!=-1])\n",
    "            Y.append(y_true[y_true!=-1])\n",
    "            Y_prob.append(y_prob[y_true!=-1])\n",
    "            if epoch%vis_every==0 and not i:\n",
    "                vis.scatter(data.pos.numpy(),Y=y_pred+2,opts=dict(webgl=False,markerborderwidth=0,markersize=5,title=\"Val Predicted {}\".format(data.id[0])),win=\"val_pred\")\n",
    "                vis.scatter(data.pos.numpy(),Y=y_true+2,opts=dict(webgl=False,markerborderwidth=0,markersize=5,title=\"Val Real {}\".format(data.id[0])),win=\"val_true\")\n",
    "                vis.scatter(PCA(n_components=2).fit_transform(z.detach().cpu().numpy()),Y=y_pred+2,opts=dict(webgl=False,markerborderwidth=0,markersize=5,title=\"Val PCA {}\".format(data.id[0])),win=\"val_pca\")\n",
    "                vis.scatter(PCA(n_components=2).fit_transform(x.detach().cpu().numpy()),Y=y_true+2,opts=dict(webgl=False,markerborderwidth=0,markersize=5,title=\"Val Original PCA {}\".format(data.id[0])),win=\"val_pca2\")\n",
    "                if use_thumbnails:\n",
    "                    vis.image(thumbnails['val'][data.index].transpose((2,1,0))[:,::-1,:],win=\"val_image\")\n",
    "            del x, edge_index, loss, y_out, z\n",
    "        val_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        val_auc=roc_auc_score(np.hstack(Y),np.vstack(Y_prob),multi_class='ovr',average='macro')\n",
    "        if save_mod and val_auc>=max(past_performance):\n",
    "            best_model_dict=copy.deepcopy(model.state_dict())\n",
    "            past_performance.append(val_auc)\n",
    "        print(epoch,train_f1,val_f1,val_auc,flush=True)\n",
    "        train_val_f1.append((train_f1,val_f1,val_auc))\n",
    "        if vis_every: vis.line(torch.tensor(np.array(train_val_f1)),win=\"f1\")\n",
    "\n",
    "    model.load_state_dict(best_model_dict)\n",
    "    torch.save(model.state_dict(),os.path.join(out_dir,model_basename))\n",
    "    torch.save(train_val_f1,os.path.join(out_dir,log_basename))\n",
    "    \n",
    "\n",
    "# class Commands(object):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "        \n",
    "#     def fit_model(self,cv_split=2,\n",
    "#                 graph_data='datasets/graph_dataset_pretrain.pkl',\n",
    "#                 cv_splits='cv_splits/cv_splits.pkl',\n",
    "#                 use_weights=False,\n",
    "#                 n_batches_backward=1,\n",
    "#                 f1_metric='weighted',\n",
    "#                 n_epochs=1500,\n",
    "#                 out_dir='models_no_pretrain',\n",
    "#                 lr=1e-2,\n",
    "#                 eta_min=1e-4,\n",
    "#                 T_max=20,\n",
    "#                 wd=0,\n",
    "#                 hidden_topology=[32,64,128,128],\n",
    "#                 p=0.5,\n",
    "#                 p2=0.3,\n",
    "#                 burnin=400,\n",
    "#                 warmup=100,\n",
    "#                 gpu_id=0,\n",
    "#                 batch_size=1,\n",
    "#                 vis_every=0,\n",
    "#                 port=5555,\n",
    "#                 num_layers=28,\n",
    "#                 thumbnails=\"bcc/image_thumbnails.pkl\",\n",
    "#                 use_thumbnails=False,\n",
    "#                 use_deep=False):\n",
    "#         fit_model(cv_split,\n",
    "#                 graph_data,\n",
    "#                 cv_splits,\n",
    "#                 use_weights,\n",
    "#                 n_batches_backward,\n",
    "#                 f1_metric,\n",
    "#                 n_epochs,\n",
    "#                 out_dir,\n",
    "#                 lr,\n",
    "#                 eta_min,\n",
    "#                 T_max,\n",
    "#                 wd,\n",
    "#                 hidden_topology,\n",
    "#                 p,\n",
    "#                 p2,\n",
    "#                 burnin,\n",
    "#                 warmup,\n",
    "#                 gpu_id,\n",
    "#                 batch_size,\n",
    "#                 vis_every,\n",
    "#                 port,\n",
    "#                 num_layers,\n",
    "#                 thumbnails,\n",
    "#                 use_thumbnails,\n",
    "#                 use_deep)\n",
    "#                 cv_split=2,\n",
    "#                 graph_data='datasets/graph_dataset_no_pretrain_final.pkl',\n",
    "#                 cv_splits='cv_splits/cv_splits_final.pkl',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os, glob\n",
    "train_val_cases=pd.read_pickle(\"updated_patches/train_val_patch_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_type='tumor'\n",
    "def check_if_exists(basename,analysis_type):\n",
    "    return len(glob.glob(os.path.join(\"graph_datasets\",analysis_type,f\"{basename}*pkl\")))>0\n",
    "cases={k:[basename for basename in train_val_cases[analysis_type][k]['ID'].unique() if check_if_exists(basename,analysis_type)] for k in ['train','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "import os, tqdm\n",
    "graph_datasets={k:reduce(add,[pd.read_pickle(os.path.join('graph_datasets',analysis_type,f'{basename}.pkl')) for basename in tqdm.tqdm(cases[k],desc='case')]) for k in tqdm.tqdm(list(cases.keys()),desc='set')}\n",
    "thumbnails={k:reduce(add,[pd.read_pickle(os.path.join('thumbnails',analysis_type,f'{basename}.pkl')) for basename in tqdm.tqdm(cases[k],desc='case')]) for k in tqdm.tqdm(list(cases.keys()),desc='set')}\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir gnn_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "225 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:241: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-371869697d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0muse_thumbnails\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0muse_deep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 )\n",
      "\u001b[0;32m<ipython-input-31-0596768c7295>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_basename, log_basename, graph_datasets, thumbnails, use_weights, n_batches_backward, f1_metric, n_epochs, out_dir, lr, eta_min, T_max, wd, hidden_topology, p, p2, burnin, warmup, gpu_id, batch_size, vis_every, port, num_layers, use_thumbnails, use_deep)\u001b[0m\n\u001b[1;32m    253\u001b[0m                      \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                      hidden_topology[0],edge_drop_p=p2,p_dropout=p,num_layers=num_layers)\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# load optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "fit_model(\n",
    "                model_basename=f\"{analysis_type}_gnn_model.pth\",\n",
    "                log_basename=f\"{analysis_type}_gnn_log.pth\",\n",
    "                graph_datasets=graph_datasets,\n",
    "                thumbnails=thumbnails,\n",
    "                use_weights=True,\n",
    "                n_batches_backward=1,\n",
    "                f1_metric='weighted',\n",
    "                n_epochs=1500,\n",
    "                out_dir='gnn_models',\n",
    "                lr=1e-2,\n",
    "                eta_min=1e-4,\n",
    "                T_max=20,\n",
    "                wd=0,\n",
    "                hidden_topology=[32,64,128,128],\n",
    "                p=0.5,\n",
    "                p2=0.3,\n",
    "                burnin=400,\n",
    "                warmup=100,\n",
    "                gpu_id=0,\n",
    "                batch_size=1,\n",
    "                vis_every=0,\n",
    "                port=5555,\n",
    "                num_layers=28,\n",
    "                use_thumbnails=False,\n",
    "                use_deep=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in graph_datasets:\n",
    "#     for i in range(len(graph_datasets[k])):\n",
    "#         graph_datasets[k][i].index=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import torch\n",
    "\n",
    "\n",
    "# y_true=np.hstack(graph.y_true.numpy() for graph in graph_datasets['train'])\n",
    "# y_true=y_true[y_true!=-1]\n",
    "# class_weight=torch.tensor(compute_class_weight('balanced',np.unique(y_true),y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_datasets/macro:\r\n",
      "108_A1c.pkl  15_A1a.pkl  37_A2eX.pkl  44_A1c.pkl  5_A1eX.pkl  85_A1b.pkl\r\n",
      "108_A1d.pkl  20_B1d.pkl  3_A2b.pkl    46_A2b.pkl  60_A1c.pkl  90_A2b.pkl\r\n",
      "125_A2b.pkl  36_B2e.pkl  3_A2c.pkl    47_A1c.pkl  60_A1d.pkl  91_A2b.pkl\r\n",
      "125_A2d.pkl  37_A1c.pkl  41_A2b.pkl   53_A1b.pkl  66_A1b.pkl  9_A7b.pkl\r\n",
      "153_A1d.pkl  37_A2d.pkl  43_A2b.pkl   5_A1d.pkl   78_A2d.pkl\r\n",
      "\r\n",
      "graph_datasets/tumor:\r\n",
      "100_A1b.pkl  120_A1a.pkl   15_A1a.pkl\t26_A1b.pkl   322_A2a.pkl  67_A1b.pkl\r\n",
      "100_B1a.pkl  120_B1b.pkl   163_A1a.pkl\t26_A2a.pkl   322_B1a.pkl  71_A1.pkl\r\n",
      "100_B1b.pkl  124_A1a.pkl   163_A1b.pkl\t26_A2b.pkl   322_B2a.pkl  75_A1a.pkl\r\n",
      "100_C1b.pkl  125_A2b.pkl   163_A1c.pkl\t282_A1a.pkl  326_A1a.pkl  75_A1b.pkl\r\n",
      "102_A1b.pkl  125_A2c.pkl   163_B2b.pkl\t286_B1b.pkl  326_A1b.pkl  86_A1a.pkl\r\n",
      "102_A1d.pkl  125_A2d.pkl   171_A1a.pkl\t287_A1a.pkl  329_A1b.pkl  88_A2a.pkl\r\n",
      "105_A1b.pkl  135_A1d.pkl   173_A1a.pkl\t28_A2a.pkl   33_A1a.pkl   95_A1b.pkl\r\n",
      "105_A2c.pkl  140_A1c.pkl   178_A1a.pkl\t291_A1c.pkl  48_A1b.pkl   97_A1a.pkl\r\n",
      "107_A1b.pkl  146_A1c.pkl   182_A1a.pkl\t293_A1.pkl   48_A2c.pkl   97_A1b.pkl\r\n",
      "107_A1c.pkl  146_A1d.pkl   189_A1a.pkl\t297_A1b.pkl  48_A2d.pkl   9_A7b.pkl\r\n",
      "117_A1a.pkl  151_A1c.pkl   190_A1b.pkl\t302_A1a.pkl  49_A1b.pkl\r\n",
      "117_A1b.pkl  151_A1eX.pkl  19_A1a.pkl\t307_A1.pkl   51_A1a.pkl\r\n",
      "117_A1c.pkl  153_A1c.pkl   219_A1a.pkl\t316_A1c.pkl  51_B1a.pkl\r\n",
      "118_A1b.pkl  153_A1d.pkl   234_A1a.pkl\t317_A1a.pkl  51_C1a.pkl\r\n",
      "11_A1c.pkl   158_A1a.pkl   264_A1.pkl\t321_A1a.pkl  65_A1b.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls graph_datasets/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
